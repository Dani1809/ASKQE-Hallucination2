{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Clona Repository"
      ],
      "metadata": {
        "id": "q2HMcftkUTzY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHmS1nY_QLgj",
        "outputId": "b97e3dfb-b9d4-42f3-d6b7-1d1d91e6a196"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ASKQE-Hallucination2'...\n",
            "remote: Enumerating objects: 356, done.\u001b[K\n",
            "remote: Counting objects: 100% (154/154), done.\u001b[K\n",
            "remote: Compressing objects: 100% (128/128), done.\u001b[K\n",
            "remote: Total 356 (delta 104), reused 26 (delta 26), pack-reused 202 (from 1)\u001b[K\n",
            "Receiving objects: 100% (356/356), 975.76 KiB | 13.55 MiB/s, done.\n",
            "Resolving deltas: 100% (204/204), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Dani1809/ASKQE-Hallucination2.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libera Memoria"
      ],
      "metadata": {
        "id": "N5pCPzHYu1Zu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TznzNBCA9ZHb",
        "outputId": "db949237-0273-4392-c80c-398141f8a44e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Free GPU: 0.00 GB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import gc\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "print(f\"Free GPU: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 50 SENTENCES SAMPLING"
      ],
      "metadata": {
        "id": "QaM2LbgZArhN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/ASKQE-Hallucination2/sampleSentences.py \\\n",
        " --input_path /content/ASKQE-Hallucination2/data/expansion_impact.jsonl \\\n",
        " --output_path /content/ASKQE-Hallucination2/data/expansion_impact_50.jsonl --n 50\n"
      ],
      "metadata": {
        "id": "6A6FLvplAqOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CONTRATICO NORMALIZATION\n"
      ],
      "metadata": {
        "id": "-nJWdTZT950y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/ASKQE-Hallucination2/normalize_contratico.py \\\n",
        "--input_path /content/ASKQE-Hallucination2/data/expansion_impact_50.jsonl \\\n",
        "--output_path /content/ASKQE-Hallucination2/data/contratico_expansion.jsonl"
      ],
      "metadata": {
        "id": "4rsQiN-Q95J4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jq_x0oN-MFPA"
      },
      "source": [
        "# BACKTRANSLATION"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -u /content/ASKQE-Hallucination2/backtranslate.py \\\n",
        "  --input_path /content/ASKQE-Hallucination2/data/contratico_expansion.jsonl \\\n",
        "  --output_path /content/ASKQE-Hallucination2/data/contratico_expansion_bt.jsonl \\\n",
        "  --source_language spa \\\n",
        "  --target_language eng\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7G5qfa_O2RSX",
        "outputId": "a1b0cf09-33b8-4a5e-d7ad-aad3cafcfdf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Using device: cuda\n",
            "[LOAD] facebook/nllb-200-distilled-600M\n",
            "config.json: 100% 846/846 [00:00<00:00, 2.25MB/s]\n",
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "tokenizer_config.json: 100% 564/564 [00:00<00:00, 3.69MB/s]\n",
            "tokenizer.json: 100% 17.3M/17.3M [00:01<00:00, 13.4MB/s]\n",
            "special_tokens_map.json: 3.55kB [00:00, 13.3MB/s]\n",
            "pytorch_model.bin: 100% 2.46G/2.46G [00:33<00:00, 74.4MB/s]\n",
            "Loading weights: 100% 512/512 [00:00<00:00, 948.87it/s, Materializing param=model.shared.weight]\n",
            "The tied weights mapping and config for this model specifies to tie model.shared.weight to lm_head.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.shared.weight to model.decoder.embed_tokens.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "The tied weights mapping and config for this model specifies to tie model.shared.weight to model.encoder.embed_tokens.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
            "generation_config.json: 100% 189/189 [00:00<00:00, 936kB/s]\n",
            "model.safetensors:   0% 0.00/2.46G [00:00<?, ?B/s][STEP] Loading dataset: /content/ASKQE-Hallucination2/data/contratico_expansion.jsonl\n",
            "[OK] JSONL with 50 samples\n",
            "model.safetensors:   1% 35.5M/2.46G [00:03<03:31, 11.5MB/s]  [1/50] BT: Examples of health restrictions you may find in some parts of China (t...\n",
            "model.safetensors:   8% 195M/2.46G [00:05<00:37, 59.6MB/s]  [2/50] BT: We have the opportunity to double data extraction per week if necessar...\n",
            "model.safetensors:  11% 262M/2.46G [00:06<00:34, 63.0MB/s]  [3/50] BT: I will send you a picture quickly...\n",
            "model.safetensors:  24% 583M/2.46G [00:11<00:19, 97.2MB/s]  [4/50] BT: Seven trials evaluated re-adapted and approved drugs for malaria, incl...\n",
            "model.safetensors:  43% 1.05G/2.46G [00:13<00:06, 229MB/s]  [5/50] BT: Finally, the early detection of a confirmed case of COVID-19 has demon...\n",
            "model.safetensors:  62% 1.52G/2.46G [00:14<00:03, 305MB/s]  [6/50] BT: Since we are adapting an epidemiological SIR model to the routine of t...\n",
            "  [7/50] BT: RCGP RSC offices involved in the annual virological surveillance of in...\n",
            "  [8/50] BT: We would not work in isolation and coordination on this investigation....\n",
            "model.safetensors:  78% 1.92G/2.46G [00:17<00:02, 208MB/s]  [9/50] BT: Most new medicines of medical interest (NCEs) fail during drug develop...\n",
            "model.safetensors:  95% 2.33G/2.46G [00:19<00:00, 223MB/s]  [10/50] BT: At the same time and separately, the RCGP RSC has expanded its surveil...\n",
            "model.safetensors: 100% 2.46G/2.46G [00:20<00:00, 122MB/s]\n",
            "  [11/50] BT: A connection to an affected area could lead to subsequent entry restri...\n",
            "  [12/50] BT: In a 2016 analysis of 106 medicines of medical interest evaluated in c...\n",
            "  [13/50] BT: It is speculated that the 1890 flu pandemic may have been caused by th...\n",
            "  [14/50] BT: Could you gently touch the other person's hand without taking a step i...\n",
            "  [15/50] BT: These people should take measures to protect themselves from COVID-19,...\n",
            "  [16/50] BT: After washing your hands and before sitting down, use disinfectant tow...\n",
            "  [17/50] BT: This could range from integrated digital learning platforms, video cla...\n",
            "  [18/50] BT: The high fever started two days ago....\n",
            "  [19/50] BT: If you have high blood pressure and diabetes,...\n",
            "  [20/50] BT: What is really known about this subject?...\n",
            "  [21/50] BT: If there are a small number of cases, this can help develop a test kit...\n",
            "  [22/50] BT: By focusing on the number of reproductions (R) (the average number of ...\n",
            "  [23/50] BT: Serious complications include pneumonia, acute respiratory distress sy...\n",
            "  [24/50] BT: The purpose of this is that, if someone on the flight is infected, the...\n",
            "  [25/50] BT: It's almost always this kind of severe chest pain....\n",
            "  [26/50] BT: The use of some kind of mask is mandatory in some countries and cities...\n",
            "  [27/50] BT: You can do so by clinical or administrative staff at the reception, ac...\n",
            "  [28/50] BT: These have a promising activity against a specific biological target r...\n",
            "  [29/50] BT: A ban on entry into apartment complexes for visitors who do not tempor...\n",
            "  [30/50] BT: Make sure that all the information and advice you receive is backed up...\n",
            "  [31/50] BT: Following Cornell's decision that students stay home after the spring ...\n",
            "  [32/50] BT: This process may include checking the pseudonymised NHS numbers of peo...\n",
            "  [33/50] BT: In order to protect public health and safety and ensure that the healt...\n",
            "  [34/50] BT: Try to reserve a seat on the side of the window and avoid moving aroun...\n",
            "  [35/50] BT: We are not aware of any significant increase in risk for the office st...\n",
            "  [36/50] BT: Possible case: \"A suspected case whose test for the virus that causes ...\n",
            "  [37/50] BT: MRNAs are genetic transcripts of the last third of the viral genome af...\n",
            "  [38/50] BT: And the symptoms don't go away in five business days....\n",
            "  [39/50] BT: Medicines of medical interest in Phase I or II trials have a low succe...\n",
            "  [40/50] BT: COVID-19 surveillance is expected to monitor epidemiological trends, q...\n",
            "  [41/50] BT: Subsequently, a UK emergency publication of SNOMED CT concepts for COV...\n",
            "  [42/50] BT: UK Government - Travel advice and restrictions for travellers from the...\n",
            "  [43/50] BT: When the transmission in the local community is significant, in additi...\n",
            "  [44/50] BT: The NHS uses the codification system of the Systematized Nomenclature ...\n",
            "  [45/50] BT: We will prepare a new application form for the offices to record recen...\n",
            "  [46/50] BT: Both drugs have interactions with prescription drugs, which affect the...\n",
            "  [47/50] BT: They also analyzed the dynamics of the spread of the flu in France dur...\n",
            "  [48/50] BT: Community mitigation strategies, which aim to curb the spread of COVID...\n",
            "  [49/50] BT: For students who do not have internet access at home, this increases t...\n",
            "  [50/50] BT: What possible consequences does this have for public health practice?...\n",
            "[INFO] Saving results to /content/ASKQE-Hallucination2/data/contratico_expansion_bt.jsonl\n",
            " BACK-TRANSLATION COMPLETED SUCCESSFULLY!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QUESTION GENERATION"
      ],
      "metadata": {
        "id": "RmszxFBzeCAu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUT7mK6L0v3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "720648d9-1251-44f0-ee7e-111b51c31be7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rconfig.json:   0% 0.00/663 [00:00<?, ?B/s]\rconfig.json: 100% 663/663 [00:00<00:00, 3.28MB/s]\n",
            "tokenizer_config.json: 7.30kB [00:00, 21.4MB/s]\n",
            "vocab.json: 2.78MB [00:00, 53.0MB/s]\n",
            "merges.txt: 1.67MB [00:00, 123MB/s]\n",
            "tokenizer.json: 7.03MB [00:00, 146MB/s]\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "model.safetensors.index.json: 27.8kB [00:00, 80.7MB/s]\n",
            "Downloading (incomplete total...): 0.00B [00:00, ?B/s]\n",
            "Downloading (incomplete total...): 100% 15.2G/15.2G [02:34<00:01, 39.6MB/s]\n",
            "Fetching 4 files: 100% 4/4 [02:34<00:00, 38.53s/it] \n",
            "Download complete: 100% 15.2G/15.2G [02:34<00:00, 98.8MB/s]\n",
            "Loading weights: 100% 339/339 [00:39<00:00,  8.65it/s, Materializing param=model.norm.weight]\n",
            "generation_config.json: 100% 243/243 [00:00<00:00, 688kB/s]\n",
            "Some parameters are on the meta device because they were offloaded to the cpu.\n",
            "[DEBUG] Input sentence:\n",
            "Examples of health restrictions you may find in some parts of China (this list is not exhaustive):\n",
            "[DEBUG] json.loads OK → type=<class 'list'>\n",
            "[DEBUG] Final questions count: 3\n",
            "[DEBUG] Record written to file\n",
            "[DEBUG] Input sentence:\n",
            "We have the opportunity to double data extraction per week if necessary and reduce costs.\n",
            "[DEBUG] json.loads OK → type=<class 'list'>\n",
            "[DEBUG] Final questions count: 4\n",
            "[DEBUG] Record written to file\n",
            "[DEBUG] Input sentence:\n",
            "I will send you a picture quickly\n",
            "[DEBUG] json.loads OK → type=<class 'list'>\n",
            "[DEBUG] Final questions count: 3\n",
            "[DEBUG] Record written to file\n",
            "[DEBUG] Input sentence:\n",
            "Seven trials evaluated re-adapted and approved drugs for malaria, including four studies on hydroxychloroquine, chlorophyll phosphate and other anti-malarial drugs.\n",
            "[DEBUG] json.loads OK → type=<class 'list'>\n",
            "[DEBUG] Final questions count: 3\n",
            "[DEBUG] Record written to file\n",
            "[DEBUG] Input sentence:\n",
            "Finally, the early detection of a confirmed case of COVID-19 has demonstrated the rapid implementation and effectiveness of this enhanced surveillance on the national network.\n",
            "[DEBUG] json.loads OK → type=<class 'list'>\n",
            "[DEBUG] Final questions count: 4\n",
            "[DEBUG] Record written to file\n",
            "[DEBUG] Input sentence:\n",
            "Since we are adapting an epidemiological SIR model to the routine of the approximate Bayesian calculation, we anticipate that our results will be solid compared to the actual data of weekly cases containing relatively small counts.\n",
            "[DEBUG] json.loads OK → type=<class 'list'>\n",
            "[DEBUG] Final questions count: 3\n",
            "[DEBUG] Record written to file\n",
            "[DEBUG] Input sentence:\n",
            "RCGP RSC offices involved in the annual virological surveillance of influenza have begun taking samples from patients with symptoms of IVRI and other respiratory diseases.\n",
            "[DEBUG] json.loads OK → type=<class 'list'>\n",
            "[DEBUG] Final questions count: 4\n",
            "[DEBUG] Record written to file\n",
            "[DEBUG] Input sentence:\n",
            "We would not work in isolation and coordination on this investigation.\n",
            "[DEBUG] json.loads OK → type=<class 'list'>\n",
            "[DEBUG] Final questions count: 3\n",
            "[DEBUG] Record written to file\n",
            "[DEBUG] Input sentence:\n",
            "Most new medicines of medical interest (NCEs) fail during drug development, either because they have an unacceptable toxicity or simply because they are not effective in treating the target disease and cause side effects, as shown in the Phase II and III clinical trials.\n",
            "[DEBUG] json.loads OK → type=<class 'list'>\n",
            "[DEBUG] Final questions count: 4\n",
            "[DEBUG] Record written to file\n",
            "[DEBUG] Input sentence:\n",
            "At the same time and separately, the RCGP RSC has expanded its surveillance to monitor the temporal and geographic distribution of COVID-19 infection in the community, as well as to evaluate the effectiveness of the containment and mitigation strategy.\n",
            "[DEBUG] json.loads OK → type=<class 'list'>\n",
            "[DEBUG] Final questions count: 3\n",
            "[DEBUG] Record written to file\n",
            "[DEBUG] Input sentence:\n",
            "A connection to an affected area could lead to subsequent entry restrictions and, if you have been to an affected area recently, some countries will not even allow you to change planes and will have to make mandatory quarantine.\n",
            "[DEBUG] json.loads OK → type=<class 'list'>\n",
            "[DEBUG] Final questions count: 3\n",
            "[DEBUG] Record written to file\n",
            "[DEBUG] Input sentence:\n",
            "In a 2016 analysis of 106 medicines of medical interest evaluated in clinical trials, the total capital expenditure for a manufacturer whose drug was approved in successful phase III trials and became a commercial success was $2.6 billion (in the dollar value in 2013), an amount that increases at an annual rate of 8.5 percent.\n",
            "[DEBUG] json.loads OK → type=<class 'list'>\n",
            "[DEBUG] Final questions count: 4\n",
            "[DEBUG] Record written to file\n",
            "[DEBUG] Input sentence:\n",
            "It is speculated that the 1890 flu pandemic may have been caused by this interspecific jump event and not by the influenza virus, due to these related factors: the time of onset, neurological symptoms, cases of pneumonia and the unknown causative agent of the pandemic.\n",
            "[DEBUG] json.loads OK → type=<class 'list'>\n",
            "[DEBUG] Final questions count: 4\n",
            "[DEBUG] Record written to file\n",
            "[DEBUG] Input sentence:\n",
            "Could you gently touch the other person's hand without taking a step in their direction?\n",
            "[DEBUG] json.loads OK → type=<class 'list'>\n",
            "[DEBUG] Final questions count: 3\n",
            "[DEBUG] Record written to file\n",
            "[DEBUG] Input sentence:\n",
            "These people should take measures to protect themselves from COVID-19, such as washing their hands, cleaning and disinfecting areas with a lot of contact, and maintaining physical distance, which includes staying at home; avoiding crowds, gatherings and travel, and avoiding contact with people who are sick, as well as wearing a mask in public places.\n",
            "[DEBUG] json.loads OK → type=<class 'list'>\n",
            "[DEBUG] Final questions count: 4\n",
            "[DEBUG] Record written to file\n",
            "[DEBUG] Input sentence:\n",
            "After washing your hands and before sitting down, use disinfectant towels to clean the area around your seat and table.\n",
            "[DEBUG] json.loads OK → type=<class 'list'>\n",
            "[DEBUG] Final questions count: 3\n",
            "[DEBUG] Record written to file\n",
            "[DEBUG] Input sentence:\n",
            "This could range from integrated digital learning platforms, video classes and CEMA to broadcast over radio, internet and television.\n",
            "[DEBUG] json.loads OK → type=<class 'list'>\n",
            "[DEBUG] Final questions count: 4\n",
            "[DEBUG] Record written to file\n",
            "[DEBUG] Input sentence:\n",
            "The high fever started two days ago.\n",
            "[DEBUG] json.loads OK → type=<class 'list'>\n",
            "[DEBUG] Final questions count: 3\n",
            "[DEBUG] Record written to file\n",
            "[DEBUG] Input sentence:\n",
            "If you have high blood pressure and diabetes,\n",
            "[DEBUG] json.loads OK → type=<class 'list'>\n",
            "[DEBUG] Final questions count: 2\n",
            "[DEBUG] Record written to file\n",
            "[DEBUG] Input sentence:\n",
            "What is really known about this subject?\n",
            "[DEBUG] json.loads OK → type=<class 'list'>\n",
            "[DEBUG] Final questions count: 1\n",
            "[DEBUG] Record written to file\n",
            "[DEBUG] Input sentence:\n",
            "If there are a small number of cases, this can help develop a test kit for patients to take to their own general practice, explore its acceptability to patients, and promote the effectiveness of treatment.\n",
            "[DEBUG] json.loads OK → type=<class 'list'>\n",
            "[DEBUG] Final questions count: 4\n",
            "[DEBUG] Record written to file\n",
            "[DEBUG] Input sentence:\n",
            "By focusing on the number of reproductions (R) (the average number of secondary cases generated by each case) and trying to reduce R to less than 1, the policy seeks to reduce the number of cases to low levels or (as seen in previous outbreaks with severe acute respiratory syndrome and Ebola) and control transmission from person to person.\n",
            "[DEBUG] json.loads OK → type=<class 'list'>\n",
            "[DEBUG] Final questions count: 4\n",
            "[DEBUG] Record written to file\n",
            "[DEBUG] Input sentence:\n",
            "Serious complications include pneumonia, acute respiratory distress syndrome, multiorgan failure and septic shock that can cause disability or death.\n",
            "[DEBUG] json.loads OK → type=<class 'list'>\n",
            "[DEBUG] Final questions count: 3\n",
            "[DEBUG] Record written to file\n",
            "[DEBUG] Input sentence:\n",
            "The purpose of this is that, if someone on the flight is infected, the authorities can follow up with the people who were sitting near this passenger and also with the rest of the passengers to test them or quarantine them.\n",
            "[DEBUG] json.loads OK → type=<class 'list'>\n",
            "[DEBUG] Final questions count: 4\n",
            "[DEBUG] Record written to file\n",
            "[DEBUG] Input sentence:\n",
            "It's almost always this kind of severe chest pain.\n",
            "[DEBUG] json.loads OK → type=<class 'list'>\n",
            "[DEBUG] Final questions count: 3\n",
            "[DEBUG] Record written to file\n",
            "[DEBUG] Input sentence:\n",
            "The use of some kind of mask is mandatory in some countries and cities to protect public health and to reduce community transmission.\n",
            "[DEBUG] json.loads OK → type=<class 'list'>\n",
            "[DEBUG] Final questions count: 4\n",
            "[DEBUG] Record written to file\n",
            "[DEBUG] Input sentence:\n",
            "You can do so by clinical or administrative staff at the reception, according to the protocol of the individual office.\n",
            "[DEBUG] json.loads OK → type=<class 'list'>\n",
            "[DEBUG] Final questions count: 3\n",
            "[DEBUG] Record written to file\n",
            "[DEBUG] Input sentence:\n",
            "These have a promising activity against a specific biological target related to the disease of COVID-19.\n",
            "[DEBUG] json.loads OK → type=<class 'list'>\n",
            "[DEBUG] Final questions count: 3\n",
            "[DEBUG] Record written to file\n",
            "[DEBUG] Input sentence:\n",
            "A ban on entry into apartment complexes for visitors who do not temporarily reside there\n",
            "[DEBUG] json.loads OK → type=<class 'list'>\n",
            "[DEBUG] Final questions count: 3\n",
            "[DEBUG] Record written to file\n",
            "[DEBUG] Input sentence:\n",
            "Make sure that all the information and advice you receive is backed up by reputable doctors, scientists and experts.\n",
            "[DEBUG] json.loads OK → type=<class 'list'>\n",
            "[DEBUG] Final questions count: 3\n",
            "[DEBUG] Record written to file\n",
            "[DEBUG] Input sentence:\n",
            "Following Cornell's decision that students stay home after the spring break and adopt virtual teaching, the mayor of Ithaca demanded \"immediate and forceful federal action: we will see a dire and unexpected economic impact as a result of the closure of Cornell University\".\n",
            "[DEBUG] json.loads OK → type=<class 'list'>\n",
            "[DEBUG] Final questions count: 4\n",
            "[DEBUG] Record written to file\n",
            "[DEBUG] Input sentence:\n",
            "This process may include checking the pseudonymised NHS numbers of people who tested positive in RCGP's RSC offices, checking the current PHE guideline on infectious considerations for confirmed cases, offering the patient an appointment following the aforementioned process and sending him a reminder via text message.\n",
            "[DEBUG] json.loads OK → type=<class 'list'>\n",
            "[DEBUG] Final questions count: 4\n",
            "[DEBUG] Record written to file\n",
            "[DEBUG] Input sentence:\n",
            "In order to protect public health and safety and ensure that the health care system can serve everyone, and prioritizing those who are at greater risk and vulnerability, all residents are ordered to immediately comply with the current state public health directives, which were developed, under my orders, by the Department of Public Health for the current statewide COVID-19 situation.\n",
            "[DEBUG] json.loads OK → type=<class 'list'>\n",
            "[DEBUG] Final questions count: 4\n",
            "[DEBUG] Record written to file\n",
            "[DEBUG] Input sentence:\n",
            "Try to reserve a seat on the side of the window and avoid moving around the cabin and talking to other passengers during the flight.\n",
            "[DEBUG] json.loads OK → type=<class 'list'>\n",
            "[DEBUG] Final questions count: 3\n",
            "[DEBUG] Record written to file\n",
            "[DEBUG] Input sentence:\n",
            "We are not aware of any significant increase in risk for the office staff or other patients due to involvement in surveillance.\n",
            "[DEBUG] json.loads OK → type=<class 'list'>\n",
            "[DEBUG] Final questions count: 3\n",
            "[DEBUG] Record written to file\n",
            "[DEBUG] Input sentence:\n",
            "Possible case: \"A suspected case whose test for the virus that causes COVID-19 is not conclusive\" or \"a suspected case for which testing could not be carried out for any reason\".\n",
            "[DEBUG] json.loads OK → type=<class 'list'>\n",
            "[DEBUG] Final questions count: 3\n",
            "[DEBUG] Record written to file\n",
            "[DEBUG] Input sentence:\n",
            "MRNAs are genetic transcripts of the last third of the viral genome after the initial overlapping reading frame along with specific proteins.\n",
            "[DEBUG] json.loads OK → type=<class 'list'>\n",
            "[DEBUG] Final questions count: 3\n",
            "[DEBUG] Record written to file\n",
            "[DEBUG] Input sentence:\n",
            "And the symptoms don't go away in five business days.\n",
            "[DEBUG] json.loads OK → type=<class 'list'>\n",
            "[DEBUG] Final questions count: 2\n",
            "[DEBUG] Record written to file\n",
            "[DEBUG] Input sentence:\n",
            "Medicines of medical interest in Phase I or II trials have a low success rate (less than 12%) in terms of passing all phases of the trial and achieving final approval.\n",
            "[DEBUG] json.loads OK → type=<class 'list'>\n",
            "[DEBUG] Final questions count: 4\n",
            "[DEBUG] Record written to file\n",
            "[DEBUG] Input sentence:\n",
            "COVID-19 surveillance is expected to monitor epidemiological trends, quickly detect new cases and, based on this accurate information, provide epidemiological data to assess risk and guide preparedness against the disease.\n",
            "[DEBUG] json.loads OK → type=<class 'list'>\n",
            "[DEBUG] Final questions count: 3\n",
            "[DEBUG] Record written to file\n",
            "[DEBUG] Input sentence:\n",
            "Subsequently, a UK emergency publication of SNOMED CT concepts for COVID-19 was also made available in all CMR systems (Table 4) and printed copies were sent to local hospitals.\n",
            "[DEBUG] json.loads OK → type=<class 'list'>\n",
            "[DEBUG] Final questions count: 4\n",
            "[DEBUG] Record written to file\n",
            "[DEBUG] Input sentence:\n",
            "UK Government - Travel advice and restrictions for travellers from the Foreign and Commonwealth Office and the Ministry of Foreign Affairs\n",
            "[DEBUG] json.loads OK → type=<class 'list'>\n",
            "[DEBUG] Final questions count: 4\n",
            "[DEBUG] Record written to file\n",
            "[DEBUG] Input sentence:\n",
            "When the transmission in the local community is significant, in addition to social distancing strategies, one may consider extending the suspension of classes and extracurricular activities.\n",
            "[DEBUG] json.loads OK → type=<class 'list'>\n",
            "[DEBUG] Final questions count: 3\n",
            "[DEBUG] Record written to file\n",
            "[DEBUG] Input sentence:\n",
            "The NHS uses the codification system of the Systematized Nomenclature of Medicine - Clinical Terms (SNOMED CT), which is usually only updated twice a year, rigorously.\n",
            "[DEBUG] json.loads OK → type=<class 'list'>\n",
            "[DEBUG] Final questions count: 3\n",
            "[DEBUG] Record written to file\n",
            "[DEBUG] Input sentence:\n",
            "We will prepare a new application form for the offices to record recent travel, work exposure and exposure to COVID-19.\n",
            "[DEBUG] json.loads OK → type=<class 'list'>\n",
            "[DEBUG] Final questions count: 3\n",
            "[DEBUG] Record written to file\n",
            "[DEBUG] Input sentence:\n",
            "Both drugs have interactions with prescription drugs, which affect the therapeutic dose, the mitigation of the disease, and the duration of treatment.\n",
            "[DEBUG] json.loads OK → type=<class 'list'>\n",
            "[DEBUG] Final questions count: 4\n",
            "[DEBUG] Record written to file\n",
            "[DEBUG] Input sentence:\n",
            "They also analyzed the dynamics of the spread of the flu in France during the French school holidays and observed that flu cases gradually decreased when schools were closed and reappeared when they reopened.\n",
            "[DEBUG] json.loads OK → type=<class 'list'>\n",
            "[DEBUG] Final questions count: 4\n",
            "[DEBUG] Record written to file\n",
            "[DEBUG] Input sentence:\n",
            "Community mitigation strategies, which aim to curb the spread of COVID-19, are important to protect all people from COVID-19, especially those with underlying medical conditions, essential workers and others at risk of serious illness associated with COVID-19 (https://www.cdc.gov/coronavirus/2019-ncov/downloads/community-mitigation-strategy.pdf).\n",
            "[DEBUG] json.loads OK → type=<class 'list'>\n",
            "[DEBUG] Final questions count: 3\n",
            "[DEBUG] Record written to file\n",
            "[DEBUG] Input sentence:\n",
            "For students who do not have internet access at home, this increases the difficulty of keeping up with distance education and doing homework online.\n",
            "[DEBUG] json.loads OK → type=<class 'list'>\n",
            "[DEBUG] Final questions count: 4\n",
            "[DEBUG] Record written to file\n",
            "[DEBUG] Input sentence:\n",
            "What possible consequences does this have for public health practice?\n",
            "[DEBUG] json.loads OK → type=<class 'list'>\n",
            "[DEBUG] Final questions count: 1\n",
            "[DEBUG] Record written to file\n",
            "\n",
            "COMPLETED — Output file written successfully.\n"
          ]
        }
      ],
      "source": [
        "!python -u /content/ASKQE-Hallucination2/qg.py \\\n",
        "  --input_path /content/ASKQE-Hallucination2/data/contratico_expansion_bt.jsonl \\\n",
        "  --output_path /content/ASKQE-Hallucination2/data/contratico_expansion_bt_qg.jsonl \\\n",
        "  --prompt_path /content/ASKQE-Hallucination2/prompt.json \\\n",
        "  --prompt_key qg_prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QUESTION ANSWERING"
      ],
      "metadata": {
        "id": "crPjiUQLvGlQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WzTipm8AcGh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "240ef271-54cc-4014-8b10-bc74c684f3ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Using device: cuda\n",
            "[INFO] Prompt key: qa_prompt\n",
            "config.json: 100% 663/663 [00:00<00:00, 1.89MB/s]\n",
            "tokenizer_config.json: 7.30kB [00:00, 28.7MB/s]\n",
            "vocab.json: 2.78MB [00:00, 115MB/s]\n",
            "merges.txt: 1.67MB [00:00, 120MB/s]\n",
            "tokenizer.json: 7.03MB [00:00, 152MB/s]\n",
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "model.safetensors.index.json: 27.8kB [00:00, 97.4MB/s]\n",
            "Downloading (incomplete total...): 0.00B [00:00, ?B/s]\n",
            "Downloading (incomplete total...): 100% 15.2G/15.2G [03:41<00:00, 50.4MB/s]\n",
            "Fetching 4 files: 100% 4/4 [03:41<00:00, 55.39s/it] \n",
            "Download complete: 100% 15.2G/15.2G [03:41<00:00, 50.4MB/s]                \n",
            "Loading weights:   0% 0/339 [00:00<?, ?it/s]\u001b[A\n",
            "Loading weights:   0% 1/339 [00:00<00:00, 12633.45it/s, Materializing param=lm_head.weight]\u001b[A\n",
            "Loading weights:   0% 1/339 [00:00<00:00, 5660.33it/s, Materializing param=lm_head.weight] \u001b[A\n",
            "Loading weights:   1% 2/339 [00:00<00:00, 4599.02it/s, Materializing param=model.embed_tokens.weight]\u001b[A\n",
            "Loading weights:   1% 2/339 [00:00<00:00, 3775.25it/s, Materializing param=model.embed_tokens.weight]\u001b[A\n",
            "Loading weights:   1% 3/339 [00:02<04:59,  1.12it/s, Materializing param=model.embed_tokens.weight]  \u001b[A\n",
            "Loading weights:   1% 3/339 [00:02<04:59,  1.12it/s, Materializing param=model.layers.0.input_layernorm.weight]\u001b[A\n",
            "Loading weights:   1% 3/339 [00:02<04:59,  1.12it/s, Materializing param=model.layers.0.input_layernorm.weight]\u001b[A\n",
            "Loading weights:   1% 4/339 [00:02<04:58,  1.12it/s, Materializing param=model.layers.0.mlp.down_proj.weight]  \u001b[A\n",
            "Loading weights:   1% 4/339 [00:02<04:58,  1.12it/s, Materializing param=model.layers.0.mlp.down_proj.weight]\u001b[A\n",
            "Loading weights:   1% 5/339 [00:02<02:40,  2.08it/s, Materializing param=model.layers.0.mlp.down_proj.weight]\u001b[A\n",
            "Loading weights:   1% 5/339 [00:02<02:40,  2.08it/s, Materializing param=model.layers.0.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:   1% 5/339 [00:02<02:40,  2.08it/s, Materializing param=model.layers.0.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:   2% 6/339 [00:02<02:40,  2.08it/s, Materializing param=model.layers.0.mlp.up_proj.weight]  \u001b[A\n",
            "Loading weights:   2% 6/339 [00:02<02:40,  2.08it/s, Materializing param=model.layers.0.mlp.up_proj.weight]\u001b[A\n",
            "Loading weights:   2% 7/339 [00:02<02:39,  2.08it/s, Materializing param=model.layers.0.post_attention_layernorm.weight]\u001b[A\n",
            "Loading weights:   2% 7/339 [00:02<02:39,  2.08it/s, Materializing param=model.layers.0.post_attention_layernorm.weight]\u001b[A\n",
            "Loading weights:   2% 8/339 [00:02<02:39,  2.08it/s, Materializing param=model.layers.0.self_attn.k_proj.bias]          \u001b[A\n",
            "Loading weights:   2% 8/339 [00:02<02:39,  2.08it/s, Materializing param=model.layers.0.self_attn.k_proj.bias]\u001b[A\n",
            "Loading weights:   3% 9/339 [00:02<02:38,  2.08it/s, Materializing param=model.layers.0.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:   3% 9/339 [00:02<02:38,  2.08it/s, Materializing param=model.layers.0.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:   3% 10/339 [00:02<02:38,  2.08it/s, Materializing param=model.layers.0.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:   3% 10/339 [00:02<02:38,  2.08it/s, Materializing param=model.layers.0.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:   3% 11/339 [00:02<02:37,  2.08it/s, Materializing param=model.layers.0.self_attn.q_proj.bias]  \u001b[A\n",
            "Loading weights:   3% 11/339 [00:02<02:37,  2.08it/s, Materializing param=model.layers.0.self_attn.q_proj.bias]\u001b[A\n",
            "Loading weights:   4% 12/339 [00:02<02:37,  2.08it/s, Materializing param=model.layers.0.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:   4% 12/339 [00:02<02:37,  2.08it/s, Materializing param=model.layers.0.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:   4% 13/339 [00:02<02:36,  2.08it/s, Materializing param=model.layers.0.self_attn.v_proj.bias]  \u001b[A\n",
            "Loading weights:   4% 13/339 [00:02<02:36,  2.08it/s, Materializing param=model.layers.0.self_attn.v_proj.bias]\u001b[A\n",
            "Loading weights:   4% 14/339 [00:02<02:36,  2.08it/s, Materializing param=model.layers.0.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:   4% 14/339 [00:02<02:36,  2.08it/s, Materializing param=model.layers.0.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:   4% 15/339 [00:02<02:35,  2.08it/s, Materializing param=model.layers.1.input_layernorm.weight] \u001b[A\n",
            "Loading weights:   4% 15/339 [00:02<02:35,  2.08it/s, Materializing param=model.layers.1.input_layernorm.weight]\u001b[A\n",
            "Loading weights:   5% 16/339 [00:02<02:35,  2.08it/s, Materializing param=model.layers.1.mlp.down_proj.weight]  \u001b[A\n",
            "Loading weights:   5% 16/339 [00:02<02:35,  2.08it/s, Materializing param=model.layers.1.mlp.down_proj.weight]\u001b[A\n",
            "Loading weights:   5% 17/339 [00:02<02:34,  2.08it/s, Materializing param=model.layers.1.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:   5% 17/339 [00:02<02:34,  2.08it/s, Materializing param=model.layers.1.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:   5% 18/339 [00:02<00:30, 10.45it/s, Materializing param=model.layers.1.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:   5% 18/339 [00:02<00:30, 10.45it/s, Materializing param=model.layers.1.mlp.up_proj.weight]  \u001b[A\n",
            "Loading weights:   5% 18/339 [00:02<00:30, 10.45it/s, Materializing param=model.layers.1.mlp.up_proj.weight]\u001b[A\n",
            "Loading weights:   6% 19/339 [00:02<00:30, 10.45it/s, Materializing param=model.layers.1.post_attention_layernorm.weight]\u001b[A\n",
            "Loading weights:   6% 19/339 [00:02<00:30, 10.45it/s, Materializing param=model.layers.1.post_attention_layernorm.weight]\u001b[A\n",
            "Loading weights:   6% 20/339 [00:02<00:30, 10.45it/s, Materializing param=model.layers.1.self_attn.k_proj.bias]          \u001b[A\n",
            "Loading weights:   6% 20/339 [00:02<00:30, 10.45it/s, Materializing param=model.layers.1.self_attn.k_proj.bias]\u001b[A\n",
            "Loading weights:   6% 21/339 [00:02<00:30, 10.45it/s, Materializing param=model.layers.1.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:   6% 21/339 [00:02<00:30, 10.45it/s, Materializing param=model.layers.1.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:   6% 22/339 [00:02<00:30, 10.45it/s, Materializing param=model.layers.1.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:   6% 22/339 [00:02<00:30, 10.45it/s, Materializing param=model.layers.1.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:   7% 23/339 [00:03<00:30, 10.45it/s, Materializing param=model.layers.1.self_attn.q_proj.bias]  \u001b[A\n",
            "Loading weights:   7% 23/339 [00:03<00:30, 10.45it/s, Materializing param=model.layers.1.self_attn.q_proj.bias]\u001b[A\n",
            "Loading weights:   7% 24/339 [00:03<00:21, 14.71it/s, Materializing param=model.layers.1.self_attn.q_proj.bias]\u001b[A\n",
            "Loading weights:   7% 24/339 [00:03<00:21, 14.71it/s, Materializing param=model.layers.1.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:   7% 24/339 [00:03<00:21, 14.71it/s, Materializing param=model.layers.1.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:   7% 25/339 [00:03<00:21, 14.71it/s, Materializing param=model.layers.1.self_attn.v_proj.bias]  \u001b[A\n",
            "Loading weights:   7% 25/339 [00:03<00:21, 14.71it/s, Materializing param=model.layers.1.self_attn.v_proj.bias]\u001b[A\n",
            "Loading weights:   8% 26/339 [00:03<00:21, 14.71it/s, Materializing param=model.layers.1.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:   8% 26/339 [00:03<00:21, 14.71it/s, Materializing param=model.layers.1.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:   8% 27/339 [00:03<00:21, 14.71it/s, Materializing param=model.layers.2.input_layernorm.weight] \u001b[A\n",
            "Loading weights:   8% 27/339 [00:03<00:21, 14.71it/s, Materializing param=model.layers.2.input_layernorm.weight]\u001b[A\n",
            "Loading weights:   8% 28/339 [00:03<00:21, 14.71it/s, Materializing param=model.layers.2.mlp.down_proj.weight]  \u001b[A\n",
            "Loading weights:   8% 28/339 [00:03<00:21, 14.71it/s, Materializing param=model.layers.2.mlp.down_proj.weight]\u001b[A\n",
            "Loading weights:   9% 29/339 [00:03<00:21, 14.71it/s, Materializing param=model.layers.2.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:   9% 29/339 [00:03<00:21, 14.71it/s, Materializing param=model.layers.2.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:   9% 30/339 [00:03<00:21, 14.71it/s, Materializing param=model.layers.2.mlp.up_proj.weight]  \u001b[A\n",
            "Loading weights:   9% 30/339 [00:03<00:21, 14.71it/s, Materializing param=model.layers.2.mlp.up_proj.weight]\u001b[A\n",
            "Loading weights:   9% 31/339 [00:03<00:15, 20.43it/s, Materializing param=model.layers.2.mlp.up_proj.weight]\u001b[A\n",
            "Loading weights:   9% 31/339 [00:03<00:15, 20.43it/s, Materializing param=model.layers.2.post_attention_layernorm.weight]\u001b[A\n",
            "Loading weights:   9% 31/339 [00:03<00:15, 20.43it/s, Materializing param=model.layers.2.post_attention_layernorm.weight]\u001b[A\n",
            "Loading weights:   9% 32/339 [00:03<00:15, 20.43it/s, Materializing param=model.layers.2.self_attn.k_proj.bias]          \u001b[A\n",
            "Loading weights:   9% 32/339 [00:03<00:15, 20.43it/s, Materializing param=model.layers.2.self_attn.k_proj.bias]\u001b[A\n",
            "Loading weights:  10% 33/339 [00:03<00:14, 20.43it/s, Materializing param=model.layers.2.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  10% 33/339 [00:03<00:14, 20.43it/s, Materializing param=model.layers.2.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  10% 34/339 [00:03<00:14, 20.43it/s, Materializing param=model.layers.2.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  10% 34/339 [00:03<00:14, 20.43it/s, Materializing param=model.layers.2.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  10% 35/339 [00:03<00:14, 20.43it/s, Materializing param=model.layers.2.self_attn.q_proj.bias]  \u001b[A\n",
            "Loading weights:  10% 35/339 [00:03<00:14, 20.43it/s, Materializing param=model.layers.2.self_attn.q_proj.bias]\u001b[A\n",
            "Loading weights:  11% 36/339 [00:03<00:14, 20.43it/s, Materializing param=model.layers.2.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  11% 36/339 [00:03<00:14, 20.43it/s, Materializing param=model.layers.2.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  11% 37/339 [00:03<00:14, 20.43it/s, Materializing param=model.layers.2.self_attn.v_proj.bias]  \u001b[A\n",
            "Loading weights:  11% 37/339 [00:03<00:14, 20.43it/s, Materializing param=model.layers.2.self_attn.v_proj.bias]\u001b[A\n",
            "Loading weights:  11% 38/339 [00:03<00:14, 20.43it/s, Materializing param=model.layers.2.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  11% 38/339 [00:03<00:14, 20.43it/s, Materializing param=model.layers.2.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  12% 39/339 [00:03<00:14, 20.43it/s, Materializing param=model.layers.3.input_layernorm.weight] \u001b[A\n",
            "Loading weights:  12% 39/339 [00:03<00:14, 20.43it/s, Materializing param=model.layers.3.input_layernorm.weight]\u001b[A\n",
            "Loading weights:  12% 40/339 [00:03<00:14, 20.43it/s, Materializing param=model.layers.3.mlp.down_proj.weight]  \u001b[A\n",
            "Loading weights:  12% 40/339 [00:03<00:14, 20.43it/s, Materializing param=model.layers.3.mlp.down_proj.weight]\u001b[A\n",
            "Loading weights:  12% 41/339 [00:03<00:14, 20.43it/s, Materializing param=model.layers.3.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:  12% 41/339 [00:03<00:14, 20.43it/s, Materializing param=model.layers.3.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:  12% 42/339 [00:03<00:09, 31.06it/s, Materializing param=model.layers.3.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:  12% 42/339 [00:03<00:09, 31.06it/s, Materializing param=model.layers.3.mlp.up_proj.weight]  \u001b[A\n",
            "Loading weights:  12% 42/339 [00:03<00:09, 31.06it/s, Materializing param=model.layers.3.mlp.up_proj.weight]\u001b[A\n",
            "Loading weights:  13% 43/339 [00:03<00:09, 31.06it/s, Materializing param=model.layers.3.post_attention_layernorm.weight]\u001b[A\n",
            "Loading weights:  13% 43/339 [00:03<00:09, 31.06it/s, Materializing param=model.layers.3.post_attention_layernorm.weight]\u001b[A\n",
            "Loading weights:  13% 44/339 [00:03<00:09, 31.06it/s, Materializing param=model.layers.3.self_attn.k_proj.bias]          \u001b[A\n",
            "Loading weights:  13% 44/339 [00:03<00:09, 31.06it/s, Materializing param=model.layers.3.self_attn.k_proj.bias]\u001b[A\n",
            "Loading weights:  13% 45/339 [00:03<00:09, 31.06it/s, Materializing param=model.layers.3.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  13% 45/339 [00:03<00:09, 31.06it/s, Materializing param=model.layers.3.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  14% 46/339 [00:03<00:09, 31.06it/s, Materializing param=model.layers.3.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  14% 46/339 [00:03<00:09, 31.06it/s, Materializing param=model.layers.3.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  14% 47/339 [00:03<00:09, 31.06it/s, Materializing param=model.layers.3.self_attn.q_proj.bias]  \u001b[A\n",
            "Loading weights:  14% 47/339 [00:03<00:09, 31.06it/s, Materializing param=model.layers.3.self_attn.q_proj.bias]\u001b[A\n",
            "Loading weights:  14% 48/339 [00:03<00:09, 31.06it/s, Materializing param=model.layers.3.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  14% 48/339 [00:03<00:09, 31.06it/s, Materializing param=model.layers.3.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  14% 49/339 [00:03<00:09, 31.06it/s, Materializing param=model.layers.3.self_attn.v_proj.bias]  \u001b[A\n",
            "Loading weights:  14% 49/339 [00:03<00:09, 31.06it/s, Materializing param=model.layers.3.self_attn.v_proj.bias]\u001b[A\n",
            "Loading weights:  15% 50/339 [00:03<00:09, 31.06it/s, Materializing param=model.layers.3.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  15% 50/339 [00:03<00:09, 31.06it/s, Materializing param=model.layers.3.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  15% 51/339 [00:03<00:09, 31.06it/s, Materializing param=model.layers.4.input_layernorm.weight] \u001b[A\n",
            "Loading weights:  15% 51/339 [00:03<00:09, 31.06it/s, Materializing param=model.layers.4.input_layernorm.weight]\u001b[A\n",
            "Loading weights:  15% 52/339 [00:03<00:09, 31.06it/s, Materializing param=model.layers.4.mlp.down_proj.weight]  \u001b[A\n",
            "Loading weights:  15% 52/339 [00:03<00:09, 31.06it/s, Materializing param=model.layers.4.mlp.down_proj.weight]\u001b[A\n",
            "Loading weights:  16% 53/339 [00:03<00:06, 43.02it/s, Materializing param=model.layers.4.mlp.down_proj.weight]\u001b[A\n",
            "Loading weights:  16% 53/339 [00:03<00:06, 43.02it/s, Materializing param=model.layers.4.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:  16% 53/339 [00:03<00:06, 43.02it/s, Materializing param=model.layers.4.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:  16% 54/339 [00:03<00:06, 43.02it/s, Materializing param=model.layers.4.mlp.up_proj.weight]  \u001b[A\n",
            "Loading weights:  16% 54/339 [00:03<00:06, 43.02it/s, Materializing param=model.layers.4.mlp.up_proj.weight]\u001b[A\n",
            "Loading weights:  16% 55/339 [00:03<00:06, 43.02it/s, Materializing param=model.layers.4.post_attention_layernorm.weight]\u001b[A\n",
            "Loading weights:  16% 55/339 [00:03<00:06, 43.02it/s, Materializing param=model.layers.4.post_attention_layernorm.weight]\u001b[A\n",
            "Loading weights:  17% 56/339 [00:03<00:06, 43.02it/s, Materializing param=model.layers.4.self_attn.k_proj.bias]          \u001b[A\n",
            "Loading weights:  17% 56/339 [00:03<00:06, 43.02it/s, Materializing param=model.layers.4.self_attn.k_proj.bias]\u001b[A\n",
            "Loading weights:  17% 57/339 [00:03<00:06, 43.02it/s, Materializing param=model.layers.4.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  17% 57/339 [00:03<00:06, 43.02it/s, Materializing param=model.layers.4.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  17% 58/339 [00:03<00:06, 43.02it/s, Materializing param=model.layers.4.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  17% 58/339 [00:03<00:06, 43.02it/s, Materializing param=model.layers.4.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  17% 59/339 [00:03<00:06, 43.02it/s, Materializing param=model.layers.4.self_attn.q_proj.bias]  \u001b[A\n",
            "Loading weights:  17% 59/339 [00:03<00:06, 43.02it/s, Materializing param=model.layers.4.self_attn.q_proj.bias]\u001b[A\n",
            "Loading weights:  18% 60/339 [00:03<00:06, 43.02it/s, Materializing param=model.layers.4.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  18% 60/339 [00:03<00:06, 43.02it/s, Materializing param=model.layers.4.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  18% 61/339 [00:03<00:05, 49.24it/s, Materializing param=model.layers.4.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  18% 61/339 [00:03<00:05, 49.24it/s, Materializing param=model.layers.4.self_attn.v_proj.bias]  \u001b[A\n",
            "Loading weights:  18% 61/339 [00:03<00:05, 49.24it/s, Materializing param=model.layers.4.self_attn.v_proj.bias]\u001b[A\n",
            "Loading weights:  18% 62/339 [00:03<00:05, 49.24it/s, Materializing param=model.layers.4.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  18% 62/339 [00:03<00:05, 49.24it/s, Materializing param=model.layers.4.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  19% 63/339 [00:03<00:05, 49.24it/s, Materializing param=model.layers.5.input_layernorm.weight] \u001b[A\n",
            "Loading weights:  19% 63/339 [00:03<00:05, 49.24it/s, Materializing param=model.layers.5.input_layernorm.weight]\u001b[A\n",
            "Loading weights:  19% 64/339 [00:03<00:05, 49.24it/s, Materializing param=model.layers.5.mlp.down_proj.weight]  \u001b[A\n",
            "Loading weights:  19% 64/339 [00:03<00:05, 49.24it/s, Materializing param=model.layers.5.mlp.down_proj.weight]\u001b[A\n",
            "Loading weights:  19% 65/339 [00:03<00:05, 49.24it/s, Materializing param=model.layers.5.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:  19% 65/339 [00:03<00:05, 49.24it/s, Materializing param=model.layers.5.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:  19% 66/339 [00:03<00:05, 49.24it/s, Materializing param=model.layers.5.mlp.up_proj.weight]  \u001b[A\n",
            "Loading weights:  19% 66/339 [00:03<00:05, 49.24it/s, Materializing param=model.layers.5.mlp.up_proj.weight]\u001b[A\n",
            "Loading weights:  20% 67/339 [00:03<00:05, 49.24it/s, Materializing param=model.layers.5.post_attention_layernorm.weight]\u001b[A\n",
            "Loading weights:  20% 67/339 [00:03<00:05, 49.24it/s, Materializing param=model.layers.5.post_attention_layernorm.weight]\u001b[A\n",
            "Loading weights:  20% 68/339 [00:03<00:05, 49.24it/s, Materializing param=model.layers.5.self_attn.k_proj.bias]          \u001b[A\n",
            "Loading weights:  20% 68/339 [00:03<00:05, 49.24it/s, Materializing param=model.layers.5.self_attn.k_proj.bias]\u001b[A\n",
            "Loading weights:  20% 69/339 [00:03<00:04, 54.63it/s, Materializing param=model.layers.5.self_attn.k_proj.bias]\u001b[A\n",
            "Loading weights:  20% 69/339 [00:03<00:04, 54.63it/s, Materializing param=model.layers.5.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  20% 69/339 [00:03<00:04, 54.63it/s, Materializing param=model.layers.5.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  21% 70/339 [00:03<00:04, 54.63it/s, Materializing param=model.layers.5.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  21% 70/339 [00:03<00:04, 54.63it/s, Materializing param=model.layers.5.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  21% 71/339 [00:03<00:04, 54.63it/s, Materializing param=model.layers.5.self_attn.q_proj.bias]  \u001b[A\n",
            "Loading weights:  21% 71/339 [00:03<00:04, 54.63it/s, Materializing param=model.layers.5.self_attn.q_proj.bias]\u001b[A\n",
            "Loading weights:  21% 72/339 [00:03<00:04, 54.63it/s, Materializing param=model.layers.5.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  21% 72/339 [00:03<00:04, 54.63it/s, Materializing param=model.layers.5.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  22% 73/339 [00:03<00:04, 54.63it/s, Materializing param=model.layers.5.self_attn.v_proj.bias]  \u001b[A\n",
            "Loading weights:  22% 73/339 [00:03<00:04, 54.63it/s, Materializing param=model.layers.5.self_attn.v_proj.bias]\u001b[A\n",
            "Loading weights:  22% 74/339 [00:03<00:04, 54.63it/s, Materializing param=model.layers.5.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  22% 74/339 [00:03<00:04, 54.63it/s, Materializing param=model.layers.5.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  22% 75/339 [00:03<00:04, 54.63it/s, Materializing param=model.layers.6.input_layernorm.weight] \u001b[A\n",
            "Loading weights:  22% 75/339 [00:03<00:04, 54.63it/s, Materializing param=model.layers.6.input_layernorm.weight]\u001b[A\n",
            "Loading weights:  22% 76/339 [00:03<00:04, 54.63it/s, Materializing param=model.layers.6.mlp.down_proj.weight]  \u001b[A\n",
            "Loading weights:  22% 76/339 [00:03<00:04, 54.63it/s, Materializing param=model.layers.6.mlp.down_proj.weight]\u001b[A\n",
            "Loading weights:  23% 77/339 [00:04<00:08, 32.41it/s, Materializing param=model.layers.6.mlp.down_proj.weight]\u001b[A\n",
            "Loading weights:  23% 77/339 [00:04<00:08, 32.41it/s, Materializing param=model.layers.6.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:  23% 77/339 [00:04<00:08, 32.41it/s, Materializing param=model.layers.6.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:  23% 78/339 [00:04<00:08, 32.41it/s, Materializing param=model.layers.6.mlp.up_proj.weight]  \u001b[A\n",
            "Loading weights:  23% 78/339 [00:04<00:08, 32.41it/s, Materializing param=model.layers.6.mlp.up_proj.weight]\u001b[A\n",
            "Loading weights:  23% 79/339 [00:05<00:08, 32.41it/s, Materializing param=model.layers.6.post_attention_layernorm.weight]\u001b[A\n",
            "Loading weights:  23% 79/339 [00:05<00:08, 32.41it/s, Materializing param=model.layers.6.post_attention_layernorm.weight]\u001b[A\n",
            "Loading weights:  24% 80/339 [00:05<00:07, 32.41it/s, Materializing param=model.layers.6.self_attn.k_proj.bias]          \u001b[A\n",
            "Loading weights:  24% 80/339 [00:05<00:07, 32.41it/s, Materializing param=model.layers.6.self_attn.k_proj.bias]\u001b[A\n",
            "Loading weights:  24% 81/339 [00:05<00:07, 32.41it/s, Materializing param=model.layers.6.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  24% 81/339 [00:05<00:07, 32.41it/s, Materializing param=model.layers.6.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  24% 82/339 [00:05<00:07, 32.41it/s, Materializing param=model.layers.6.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  24% 82/339 [00:05<00:07, 32.41it/s, Materializing param=model.layers.6.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  24% 83/339 [00:05<00:17, 14.82it/s, Materializing param=model.layers.6.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  24% 83/339 [00:05<00:17, 14.82it/s, Materializing param=model.layers.6.self_attn.q_proj.bias]  \u001b[A\n",
            "Loading weights:  24% 83/339 [00:05<00:17, 14.82it/s, Materializing param=model.layers.6.self_attn.q_proj.bias]\u001b[A\n",
            "Loading weights:  25% 84/339 [00:05<00:17, 14.82it/s, Materializing param=model.layers.6.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  25% 84/339 [00:05<00:17, 14.82it/s, Materializing param=model.layers.6.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  25% 85/339 [00:05<00:17, 14.82it/s, Materializing param=model.layers.6.self_attn.v_proj.bias]  \u001b[A\n",
            "Loading weights:  25% 85/339 [00:05<00:17, 14.82it/s, Materializing param=model.layers.6.self_attn.v_proj.bias]\u001b[A\n",
            "Loading weights:  25% 86/339 [00:05<00:17, 14.82it/s, Materializing param=model.layers.6.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  25% 86/339 [00:05<00:17, 14.82it/s, Materializing param=model.layers.6.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  26% 87/339 [00:05<00:17, 14.82it/s, Materializing param=model.layers.7.input_layernorm.weight] \u001b[A\n",
            "Loading weights:  26% 87/339 [00:05<00:17, 14.82it/s, Materializing param=model.layers.7.input_layernorm.weight]\u001b[A\n",
            "Loading weights:  26% 88/339 [00:05<00:16, 14.82it/s, Materializing param=model.layers.7.mlp.down_proj.weight]  \u001b[A\n",
            "Loading weights:  26% 88/339 [00:05<00:16, 14.82it/s, Materializing param=model.layers.7.mlp.down_proj.weight]\u001b[A\n",
            "Loading weights:  26% 89/339 [00:05<00:18, 13.27it/s, Materializing param=model.layers.7.mlp.down_proj.weight]\u001b[A\n",
            "Loading weights:  26% 89/339 [00:05<00:18, 13.27it/s, Materializing param=model.layers.7.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:  26% 89/339 [00:05<00:18, 13.27it/s, Materializing param=model.layers.7.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:  27% 90/339 [00:06<00:18, 13.27it/s, Materializing param=model.layers.7.mlp.up_proj.weight]  \u001b[A\n",
            "Loading weights:  27% 90/339 [00:06<00:18, 13.27it/s, Materializing param=model.layers.7.mlp.up_proj.weight]\u001b[A\n",
            "Loading weights:  27% 91/339 [00:06<00:18, 13.27it/s, Materializing param=model.layers.7.post_attention_layernorm.weight]\u001b[A\n",
            "Loading weights:  27% 91/339 [00:06<00:18, 13.27it/s, Materializing param=model.layers.7.post_attention_layernorm.weight]\u001b[A\n",
            "Loading weights:  27% 92/339 [00:06<00:18, 13.27it/s, Materializing param=model.layers.7.self_attn.k_proj.bias]          \u001b[A\n",
            "Loading weights:  27% 92/339 [00:06<00:18, 13.27it/s, Materializing param=model.layers.7.self_attn.k_proj.bias]\u001b[A\n",
            "Loading weights:  27% 93/339 [00:06<00:28,  8.75it/s, Materializing param=model.layers.7.self_attn.k_proj.bias]\u001b[A\n",
            "Loading weights:  27% 93/339 [00:06<00:28,  8.75it/s, Materializing param=model.layers.7.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  27% 93/339 [00:06<00:28,  8.75it/s, Materializing param=model.layers.7.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  28% 94/339 [00:06<00:27,  8.75it/s, Materializing param=model.layers.7.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  28% 94/339 [00:06<00:27,  8.75it/s, Materializing param=model.layers.7.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  28% 95/339 [00:06<00:27,  8.75it/s, Materializing param=model.layers.7.self_attn.q_proj.bias]  \u001b[A\n",
            "Loading weights:  28% 95/339 [00:06<00:27,  8.75it/s, Materializing param=model.layers.7.self_attn.q_proj.bias]\u001b[A\n",
            "Loading weights:  28% 96/339 [00:06<00:24,  9.94it/s, Materializing param=model.layers.7.self_attn.q_proj.bias]\u001b[A\n",
            "Loading weights:  28% 96/339 [00:06<00:24,  9.94it/s, Materializing param=model.layers.7.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  28% 96/339 [00:06<00:24,  9.94it/s, Materializing param=model.layers.7.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  29% 97/339 [00:07<00:24,  9.94it/s, Materializing param=model.layers.7.self_attn.v_proj.bias]  \u001b[A\n",
            "Loading weights:  29% 97/339 [00:07<00:24,  9.94it/s, Materializing param=model.layers.7.self_attn.v_proj.bias]\u001b[A\n",
            "Loading weights:  29% 98/339 [00:07<00:24,  9.94it/s, Materializing param=model.layers.7.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  29% 98/339 [00:07<00:24,  9.94it/s, Materializing param=model.layers.7.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  29% 99/339 [00:07<00:21, 11.32it/s, Materializing param=model.layers.7.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  29% 99/339 [00:07<00:21, 11.32it/s, Materializing param=model.layers.8.input_layernorm.weight] \u001b[A\n",
            "Loading weights:  29% 99/339 [00:07<00:21, 11.32it/s, Materializing param=model.layers.8.input_layernorm.weight]\u001b[A\n",
            "Loading weights:  29% 100/339 [00:07<00:21, 11.32it/s, Materializing param=model.layers.8.mlp.down_proj.weight] \u001b[A\n",
            "Loading weights:  29% 100/339 [00:07<00:21, 11.32it/s, Materializing param=model.layers.8.mlp.down_proj.weight]\u001b[A\n",
            "Loading weights:  30% 101/339 [00:07<00:21, 11.32it/s, Materializing param=model.layers.8.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:  30% 101/339 [00:07<00:21, 11.32it/s, Materializing param=model.layers.8.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:  30% 102/339 [00:08<00:35,  6.72it/s, Materializing param=model.layers.8.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:  30% 102/339 [00:08<00:35,  6.72it/s, Materializing param=model.layers.8.mlp.up_proj.weight]  \u001b[A\n",
            "Loading weights:  30% 102/339 [00:08<00:35,  6.72it/s, Materializing param=model.layers.8.mlp.up_proj.weight]\u001b[A\n",
            "Loading weights:  30% 103/339 [00:08<00:35,  6.72it/s, Materializing param=model.layers.8.post_attention_layernorm.weight]\u001b[A\n",
            "Loading weights:  30% 103/339 [00:08<00:35,  6.72it/s, Materializing param=model.layers.8.post_attention_layernorm.weight]\u001b[A\n",
            "Loading weights:  31% 104/339 [00:08<00:39,  5.89it/s, Materializing param=model.layers.8.post_attention_layernorm.weight]\u001b[A\n",
            "Loading weights:  31% 104/339 [00:08<00:39,  5.89it/s, Materializing param=model.layers.8.self_attn.k_proj.bias]          \u001b[A\n",
            "Loading weights:  31% 104/339 [00:08<00:39,  5.89it/s, Materializing param=model.layers.8.self_attn.k_proj.bias]\u001b[A\n",
            "Loading weights:  31% 105/339 [00:08<00:39,  5.89it/s, Materializing param=model.layers.8.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  31% 105/339 [00:08<00:39,  5.89it/s, Materializing param=model.layers.8.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  31% 106/339 [00:08<00:39,  5.89it/s, Materializing param=model.layers.8.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  31% 106/339 [00:08<00:39,  5.89it/s, Materializing param=model.layers.8.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  32% 107/339 [00:08<00:31,  7.42it/s, Materializing param=model.layers.8.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  32% 107/339 [00:08<00:31,  7.42it/s, Materializing param=model.layers.8.self_attn.q_proj.bias]  \u001b[A\n",
            "Loading weights:  32% 107/339 [00:08<00:31,  7.42it/s, Materializing param=model.layers.8.self_attn.q_proj.bias]\u001b[A\n",
            "Loading weights:  32% 108/339 [00:08<00:31,  7.42it/s, Materializing param=model.layers.8.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  32% 108/339 [00:08<00:31,  7.42it/s, Materializing param=model.layers.8.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  32% 109/339 [00:08<00:27,  8.51it/s, Materializing param=model.layers.8.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  32% 109/339 [00:08<00:27,  8.51it/s, Materializing param=model.layers.8.self_attn.v_proj.bias]  \u001b[A\n",
            "Loading weights:  32% 109/339 [00:08<00:27,  8.51it/s, Materializing param=model.layers.8.self_attn.v_proj.bias]\u001b[A\n",
            "Loading weights:  32% 110/339 [00:08<00:26,  8.51it/s, Materializing param=model.layers.8.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  32% 110/339 [00:08<00:26,  8.51it/s, Materializing param=model.layers.8.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  33% 111/339 [00:08<00:26,  8.51it/s, Materializing param=model.layers.9.input_layernorm.weight] \u001b[A\n",
            "Loading weights:  33% 111/339 [00:08<00:26,  8.51it/s, Materializing param=model.layers.9.input_layernorm.weight]\u001b[A\n",
            "Loading weights:  33% 112/339 [00:08<00:26,  8.51it/s, Materializing param=model.layers.9.mlp.down_proj.weight]  \u001b[A\n",
            "Loading weights:  33% 112/339 [00:08<00:26,  8.51it/s, Materializing param=model.layers.9.mlp.down_proj.weight]\u001b[A\n",
            "Loading weights:  33% 113/339 [00:09<00:28,  7.95it/s, Materializing param=model.layers.9.mlp.down_proj.weight]\u001b[A\n",
            "Loading weights:  33% 113/339 [00:09<00:28,  7.95it/s, Materializing param=model.layers.9.mlp.gate_proj.weight]\u001b[A\n",
            "Download complete: 100% 15.2G/15.2G [03:52<00:00, 50.4MB/s]\n",
            "Loading weights:  34% 114/339 [00:10<00:28,  7.95it/s, Materializing param=model.layers.9.mlp.up_proj.weight]  \u001b[A\n",
            "Loading weights:  34% 114/339 [00:10<00:28,  7.95it/s, Materializing param=model.layers.9.mlp.up_proj.weight]\u001b[A\n",
            "Loading weights:  34% 115/339 [00:10<00:47,  4.70it/s, Materializing param=model.layers.9.mlp.up_proj.weight]\u001b[A\n",
            "Loading weights:  34% 115/339 [00:10<00:47,  4.70it/s, Materializing param=model.layers.9.post_attention_layernorm.weight]\u001b[A\n",
            "Loading weights:  34% 115/339 [00:10<00:47,  4.70it/s, Materializing param=model.layers.9.post_attention_layernorm.weight]\u001b[A\n",
            "Loading weights:  34% 116/339 [00:10<00:47,  4.70it/s, Materializing param=model.layers.9.self_attn.k_proj.bias]          \u001b[A\n",
            "Loading weights:  34% 116/339 [00:10<00:47,  4.70it/s, Materializing param=model.layers.9.self_attn.k_proj.bias]\u001b[A\n",
            "Loading weights:  35% 117/339 [00:10<00:39,  5.64it/s, Materializing param=model.layers.9.self_attn.k_proj.bias]\u001b[A\n",
            "Loading weights:  35% 117/339 [00:10<00:39,  5.64it/s, Materializing param=model.layers.9.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  35% 117/339 [00:10<00:39,  5.64it/s, Materializing param=model.layers.9.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  35% 118/339 [00:10<00:39,  5.64it/s, Materializing param=model.layers.9.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  35% 118/339 [00:10<00:39,  5.64it/s, Materializing param=model.layers.9.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  35% 119/339 [00:10<00:39,  5.64it/s, Materializing param=model.layers.9.self_attn.q_proj.bias]  \u001b[A\n",
            "Loading weights:  35% 119/339 [00:10<00:39,  5.64it/s, Materializing param=model.layers.9.self_attn.q_proj.bias]\u001b[A\n",
            "Loading weights:  35% 120/339 [00:10<00:38,  5.64it/s, Materializing param=model.layers.9.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  35% 120/339 [00:10<00:38,  5.64it/s, Materializing param=model.layers.9.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  36% 121/339 [00:10<00:38,  5.64it/s, Materializing param=model.layers.9.self_attn.v_proj.bias]  \u001b[A\n",
            "Loading weights:  36% 121/339 [00:10<00:38,  5.64it/s, Materializing param=model.layers.9.self_attn.v_proj.bias]\u001b[A\n",
            "Loading weights:  36% 122/339 [00:10<00:38,  5.64it/s, Materializing param=model.layers.9.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  36% 122/339 [00:10<00:38,  5.64it/s, Materializing param=model.layers.9.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  36% 123/339 [00:10<00:21, 10.11it/s, Materializing param=model.layers.9.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  36% 123/339 [00:10<00:21, 10.11it/s, Materializing param=model.layers.10.input_layernorm.weight]\u001b[A\n",
            "Loading weights:  36% 123/339 [00:10<00:21, 10.11it/s, Materializing param=model.layers.10.input_layernorm.weight]\u001b[A\n",
            "Loading weights:  37% 124/339 [00:10<00:21, 10.11it/s, Materializing param=model.layers.10.mlp.down_proj.weight]  \u001b[A\n",
            "Loading weights:  37% 124/339 [00:10<00:21, 10.11it/s, Materializing param=model.layers.10.mlp.down_proj.weight]\u001b[A\n",
            "Loading weights:  37% 125/339 [00:11<00:21, 10.11it/s, Materializing param=model.layers.10.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:  37% 125/339 [00:11<00:21, 10.11it/s, Materializing param=model.layers.10.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:  37% 126/339 [00:11<00:35,  5.94it/s, Materializing param=model.layers.10.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:  37% 126/339 [00:11<00:35,  5.94it/s, Materializing param=model.layers.10.mlp.up_proj.weight]  \u001b[A\n",
            "Loading weights:  37% 126/339 [00:11<00:35,  5.94it/s, Materializing param=model.layers.10.mlp.up_proj.weight]\u001b[A\n",
            "Loading weights:  37% 127/339 [00:12<00:35,  5.94it/s, Materializing param=model.layers.10.post_attention_layernorm.weight]\u001b[A\n",
            "Loading weights:  37% 127/339 [00:12<00:35,  5.94it/s, Materializing param=model.layers.10.post_attention_layernorm.weight]\u001b[A\n",
            "Loading weights:  38% 128/339 [00:12<00:39,  5.30it/s, Materializing param=model.layers.10.post_attention_layernorm.weight]\u001b[A\n",
            "Loading weights:  38% 128/339 [00:12<00:39,  5.30it/s, Materializing param=model.layers.10.self_attn.k_proj.bias]          \u001b[A\n",
            "Loading weights:  38% 128/339 [00:12<00:39,  5.30it/s, Materializing param=model.layers.10.self_attn.k_proj.bias]\u001b[A\n",
            "Loading weights:  38% 129/339 [00:12<00:39,  5.30it/s, Materializing param=model.layers.10.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  38% 129/339 [00:12<00:39,  5.30it/s, Materializing param=model.layers.10.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  38% 130/339 [00:12<00:39,  5.30it/s, Materializing param=model.layers.10.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  38% 130/339 [00:12<00:39,  5.30it/s, Materializing param=model.layers.10.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  39% 131/339 [00:12<00:30,  6.92it/s, Materializing param=model.layers.10.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  39% 131/339 [00:12<00:30,  6.92it/s, Materializing param=model.layers.10.self_attn.q_proj.bias]  \u001b[A\n",
            "Loading weights:  39% 131/339 [00:12<00:30,  6.92it/s, Materializing param=model.layers.10.self_attn.q_proj.bias]\u001b[A\n",
            "Loading weights:  39% 132/339 [00:12<00:29,  6.92it/s, Materializing param=model.layers.10.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  39% 132/339 [00:12<00:29,  6.92it/s, Materializing param=model.layers.10.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  39% 133/339 [00:12<00:29,  6.92it/s, Materializing param=model.layers.10.self_attn.v_proj.bias]  \u001b[A\n",
            "Loading weights:  39% 133/339 [00:12<00:29,  6.92it/s, Materializing param=model.layers.10.self_attn.v_proj.bias]\u001b[A\n",
            "Loading weights:  40% 134/339 [00:12<00:22,  8.96it/s, Materializing param=model.layers.10.self_attn.v_proj.bias]\u001b[A\n",
            "Loading weights:  40% 134/339 [00:12<00:22,  8.96it/s, Materializing param=model.layers.10.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  40% 134/339 [00:12<00:22,  8.96it/s, Materializing param=model.layers.10.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  40% 135/339 [00:12<00:22,  8.96it/s, Materializing param=model.layers.11.input_layernorm.weight] \u001b[A\n",
            "Loading weights:  40% 135/339 [00:12<00:22,  8.96it/s, Materializing param=model.layers.11.input_layernorm.weight]\u001b[A\n",
            "Loading weights:  40% 136/339 [00:12<00:22,  8.96it/s, Materializing param=model.layers.11.mlp.down_proj.weight]  \u001b[A\n",
            "Loading weights:  40% 136/339 [00:12<00:22,  8.96it/s, Materializing param=model.layers.11.mlp.down_proj.weight]\u001b[A\n",
            "Loading weights:  40% 137/339 [00:13<00:27,  7.48it/s, Materializing param=model.layers.11.mlp.down_proj.weight]\u001b[A\n",
            "Loading weights:  40% 137/339 [00:13<00:27,  7.48it/s, Materializing param=model.layers.11.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:  40% 137/339 [00:13<00:27,  7.48it/s, Materializing param=model.layers.11.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:  41% 138/339 [00:13<00:26,  7.48it/s, Materializing param=model.layers.11.mlp.up_proj.weight]  \u001b[A\n",
            "Loading weights:  41% 138/339 [00:13<00:26,  7.48it/s, Materializing param=model.layers.11.mlp.up_proj.weight]\u001b[A\n",
            "Loading weights:  41% 139/339 [00:14<00:44,  4.47it/s, Materializing param=model.layers.11.mlp.up_proj.weight]\u001b[A\n",
            "Loading weights:  41% 139/339 [00:14<00:44,  4.47it/s, Materializing param=model.layers.11.post_attention_layernorm.weight]\u001b[A\n",
            "Loading weights:  41% 139/339 [00:14<00:44,  4.47it/s, Materializing param=model.layers.11.post_attention_layernorm.weight]\u001b[A\n",
            "Loading weights:  41% 140/339 [00:14<00:44,  4.47it/s, Materializing param=model.layers.11.self_attn.k_proj.bias]          \u001b[A\n",
            "Loading weights:  41% 140/339 [00:14<00:44,  4.47it/s, Materializing param=model.layers.11.self_attn.k_proj.bias]\u001b[A\n",
            "Loading weights:  42% 141/339 [00:14<00:44,  4.47it/s, Materializing param=model.layers.11.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  42% 141/339 [00:14<00:44,  4.47it/s, Materializing param=model.layers.11.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  42% 142/339 [00:14<00:44,  4.47it/s, Materializing param=model.layers.11.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  42% 142/339 [00:14<00:44,  4.47it/s, Materializing param=model.layers.11.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  42% 143/339 [00:14<00:29,  6.73it/s, Materializing param=model.layers.11.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  42% 143/339 [00:14<00:29,  6.73it/s, Materializing param=model.layers.11.self_attn.q_proj.bias]  \u001b[A\n",
            "Loading weights:  42% 143/339 [00:14<00:29,  6.73it/s, Materializing param=model.layers.11.self_attn.q_proj.bias]\u001b[A\n",
            "Loading weights:  42% 144/339 [00:14<00:28,  6.73it/s, Materializing param=model.layers.11.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  42% 144/339 [00:14<00:28,  6.73it/s, Materializing param=model.layers.11.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  43% 145/339 [00:14<00:24,  7.86it/s, Materializing param=model.layers.11.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  43% 145/339 [00:14<00:24,  7.86it/s, Materializing param=model.layers.11.self_attn.v_proj.bias]  \u001b[A\n",
            "Loading weights:  43% 145/339 [00:14<00:24,  7.86it/s, Materializing param=model.layers.11.self_attn.v_proj.bias]\u001b[A\n",
            "Loading weights:  43% 146/339 [00:14<00:24,  7.86it/s, Materializing param=model.layers.11.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  43% 146/339 [00:14<00:24,  7.86it/s, Materializing param=model.layers.11.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  43% 147/339 [00:14<00:24,  7.86it/s, Materializing param=model.layers.12.input_layernorm.weight] \u001b[A\n",
            "Loading weights:  43% 147/339 [00:14<00:24,  7.86it/s, Materializing param=model.layers.12.input_layernorm.weight]\u001b[A\n",
            "Loading weights:  44% 148/339 [00:14<00:24,  7.86it/s, Materializing param=model.layers.12.mlp.down_proj.weight]  \u001b[A\n",
            "Loading weights:  44% 148/339 [00:14<00:24,  7.86it/s, Materializing param=model.layers.12.mlp.down_proj.weight]\u001b[A\n",
            "Loading weights:  44% 149/339 [00:15<00:25,  7.58it/s, Materializing param=model.layers.12.mlp.down_proj.weight]\u001b[A\n",
            "Loading weights:  44% 149/339 [00:15<00:25,  7.58it/s, Materializing param=model.layers.12.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:  44% 149/339 [00:15<00:25,  7.58it/s, Materializing param=model.layers.12.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:  44% 150/339 [00:15<00:24,  7.58it/s, Materializing param=model.layers.12.mlp.up_proj.weight]  \u001b[A\n",
            "Loading weights:  44% 150/339 [00:15<00:24,  7.58it/s, Materializing param=model.layers.12.mlp.up_proj.weight]\u001b[A\n",
            "Loading weights:  45% 151/339 [00:16<00:41,  4.55it/s, Materializing param=model.layers.12.mlp.up_proj.weight]\u001b[A\n",
            "Loading weights:  45% 151/339 [00:16<00:41,  4.55it/s, Materializing param=model.layers.12.post_attention_layernorm.weight]\u001b[A\n",
            "Loading weights:  45% 151/339 [00:16<00:41,  4.55it/s, Materializing param=model.layers.12.post_attention_layernorm.weight]\u001b[A\n",
            "Loading weights:  45% 152/339 [00:16<00:41,  4.55it/s, Materializing param=model.layers.12.self_attn.k_proj.bias]          \u001b[A\n",
            "Loading weights:  45% 152/339 [00:16<00:41,  4.55it/s, Materializing param=model.layers.12.self_attn.k_proj.bias]\u001b[A\n",
            "Loading weights:  45% 153/339 [00:16<00:40,  4.55it/s, Materializing param=model.layers.12.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  45% 153/339 [00:16<00:40,  4.55it/s, Materializing param=model.layers.12.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  45% 154/339 [00:16<00:40,  4.55it/s, Materializing param=model.layers.12.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  45% 154/339 [00:16<00:40,  4.55it/s, Materializing param=model.layers.12.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  46% 155/339 [00:16<00:27,  6.78it/s, Materializing param=model.layers.12.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  46% 155/339 [00:16<00:27,  6.78it/s, Materializing param=model.layers.12.self_attn.q_proj.bias]  \u001b[A\n",
            "Loading weights:  46% 155/339 [00:16<00:27,  6.78it/s, Materializing param=model.layers.12.self_attn.q_proj.bias]\u001b[A\n",
            "Loading weights:  46% 156/339 [00:16<00:26,  6.78it/s, Materializing param=model.layers.12.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  46% 156/339 [00:16<00:26,  6.78it/s, Materializing param=model.layers.12.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  46% 157/339 [00:16<00:23,  7.85it/s, Materializing param=model.layers.12.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  46% 157/339 [00:16<00:23,  7.85it/s, Materializing param=model.layers.12.self_attn.v_proj.bias]  \u001b[A\n",
            "Loading weights:  46% 157/339 [00:16<00:23,  7.85it/s, Materializing param=model.layers.12.self_attn.v_proj.bias]\u001b[A\n",
            "Loading weights:  47% 158/339 [00:16<00:23,  7.85it/s, Materializing param=model.layers.12.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  47% 158/339 [00:16<00:23,  7.85it/s, Materializing param=model.layers.12.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  47% 159/339 [00:16<00:22,  7.85it/s, Materializing param=model.layers.13.input_layernorm.weight] \u001b[A\n",
            "Loading weights:  47% 159/339 [00:16<00:22,  7.85it/s, Materializing param=model.layers.13.input_layernorm.weight]\u001b[A\n",
            "Loading weights:  47% 160/339 [00:16<00:22,  7.85it/s, Materializing param=model.layers.13.mlp.down_proj.weight]  \u001b[A\n",
            "Loading weights:  47% 160/339 [00:16<00:22,  7.85it/s, Materializing param=model.layers.13.mlp.down_proj.weight]\u001b[A\n",
            "Loading weights:  47% 161/339 [00:16<00:23,  7.57it/s, Materializing param=model.layers.13.mlp.down_proj.weight]\u001b[A\n",
            "Loading weights:  47% 161/339 [00:16<00:23,  7.57it/s, Materializing param=model.layers.13.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:  47% 161/339 [00:16<00:23,  7.57it/s, Materializing param=model.layers.13.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:  48% 162/339 [00:17<00:23,  7.57it/s, Materializing param=model.layers.13.mlp.up_proj.weight]  \u001b[A\n",
            "Loading weights:  48% 162/339 [00:17<00:23,  7.57it/s, Materializing param=model.layers.13.mlp.up_proj.weight]\u001b[A\n",
            "Loading weights:  48% 163/339 [00:18<00:38,  4.57it/s, Materializing param=model.layers.13.mlp.up_proj.weight]\u001b[A\n",
            "Loading weights:  48% 163/339 [00:18<00:38,  4.57it/s, Materializing param=model.layers.13.post_attention_layernorm.weight]\u001b[A\n",
            "Loading weights:  48% 163/339 [00:18<00:38,  4.57it/s, Materializing param=model.layers.13.post_attention_layernorm.weight]\u001b[A\n",
            "Loading weights:  48% 164/339 [00:18<00:38,  4.57it/s, Materializing param=model.layers.13.self_attn.k_proj.bias]          \u001b[A\n",
            "Loading weights:  48% 164/339 [00:18<00:38,  4.57it/s, Materializing param=model.layers.13.self_attn.k_proj.bias]\u001b[A\n",
            "Loading weights:  49% 165/339 [00:18<00:38,  4.57it/s, Materializing param=model.layers.13.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  49% 165/339 [00:18<00:38,  4.57it/s, Materializing param=model.layers.13.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  49% 166/339 [00:18<00:37,  4.57it/s, Materializing param=model.layers.13.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  49% 166/339 [00:18<00:37,  4.57it/s, Materializing param=model.layers.13.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  49% 167/339 [00:18<00:25,  6.79it/s, Materializing param=model.layers.13.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  49% 167/339 [00:18<00:25,  6.79it/s, Materializing param=model.layers.13.self_attn.q_proj.bias]  \u001b[A\n",
            "Loading weights:  49% 167/339 [00:18<00:25,  6.79it/s, Materializing param=model.layers.13.self_attn.q_proj.bias]\u001b[A\n",
            "Loading weights:  50% 168/339 [00:18<00:25,  6.79it/s, Materializing param=model.layers.13.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  50% 168/339 [00:18<00:25,  6.79it/s, Materializing param=model.layers.13.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  50% 169/339 [00:18<00:21,  7.86it/s, Materializing param=model.layers.13.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  50% 169/339 [00:18<00:21,  7.86it/s, Materializing param=model.layers.13.self_attn.v_proj.bias]  \u001b[A\n",
            "Loading weights:  50% 169/339 [00:18<00:21,  7.86it/s, Materializing param=model.layers.13.self_attn.v_proj.bias]\u001b[A\n",
            "Loading weights:  50% 170/339 [00:18<00:21,  7.86it/s, Materializing param=model.layers.13.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  50% 170/339 [00:18<00:21,  7.86it/s, Materializing param=model.layers.13.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  50% 171/339 [00:18<00:21,  7.86it/s, Materializing param=model.layers.14.input_layernorm.weight] \u001b[A\n",
            "Loading weights:  50% 171/339 [00:18<00:21,  7.86it/s, Materializing param=model.layers.14.input_layernorm.weight]\u001b[A\n",
            "Loading weights:  51% 172/339 [00:18<00:21,  7.86it/s, Materializing param=model.layers.14.mlp.down_proj.weight]  \u001b[A\n",
            "Loading weights:  51% 172/339 [00:18<00:21,  7.86it/s, Materializing param=model.layers.14.mlp.down_proj.weight]\u001b[A\n",
            "Loading weights:  51% 173/339 [00:18<00:21,  7.62it/s, Materializing param=model.layers.14.mlp.down_proj.weight]\u001b[A\n",
            "Loading weights:  51% 173/339 [00:18<00:21,  7.62it/s, Materializing param=model.layers.14.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:  51% 173/339 [00:18<00:21,  7.62it/s, Materializing param=model.layers.14.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:  51% 174/339 [00:19<00:21,  7.62it/s, Materializing param=model.layers.14.mlp.up_proj.weight]  \u001b[A\n",
            "Loading weights:  51% 174/339 [00:19<00:21,  7.62it/s, Materializing param=model.layers.14.mlp.up_proj.weight]\u001b[A\n",
            "Loading weights:  52% 175/339 [00:19<00:35,  4.57it/s, Materializing param=model.layers.14.mlp.up_proj.weight]\u001b[A\n",
            "Loading weights:  52% 175/339 [00:19<00:35,  4.57it/s, Materializing param=model.layers.14.post_attention_layernorm.weight]\u001b[A\n",
            "Loading weights:  52% 175/339 [00:19<00:35,  4.57it/s, Materializing param=model.layers.14.post_attention_layernorm.weight]\u001b[A\n",
            "Loading weights:  52% 176/339 [00:19<00:35,  4.57it/s, Materializing param=model.layers.14.self_attn.k_proj.bias]          \u001b[A\n",
            "Loading weights:  52% 176/339 [00:19<00:35,  4.57it/s, Materializing param=model.layers.14.self_attn.k_proj.bias]\u001b[A\n",
            "Loading weights:  52% 177/339 [00:19<00:35,  4.57it/s, Materializing param=model.layers.14.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  52% 177/339 [00:19<00:35,  4.57it/s, Materializing param=model.layers.14.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  53% 178/339 [00:19<00:35,  4.57it/s, Materializing param=model.layers.14.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  53% 178/339 [00:19<00:35,  4.57it/s, Materializing param=model.layers.14.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  53% 179/339 [00:19<00:23,  6.80it/s, Materializing param=model.layers.14.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  53% 179/339 [00:19<00:23,  6.80it/s, Materializing param=model.layers.14.self_attn.q_proj.bias]  \u001b[A\n",
            "Loading weights:  53% 179/339 [00:19<00:23,  6.80it/s, Materializing param=model.layers.14.self_attn.q_proj.bias]\u001b[A\n",
            "Loading weights:  53% 180/339 [00:19<00:23,  6.80it/s, Materializing param=model.layers.14.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  53% 180/339 [00:19<00:23,  6.80it/s, Materializing param=model.layers.14.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  53% 181/339 [00:20<00:19,  7.90it/s, Materializing param=model.layers.14.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  53% 181/339 [00:20<00:19,  7.90it/s, Materializing param=model.layers.14.self_attn.v_proj.bias]  \u001b[A\n",
            "Loading weights:  53% 181/339 [00:20<00:19,  7.90it/s, Materializing param=model.layers.14.self_attn.v_proj.bias]\u001b[A\n",
            "Loading weights:  54% 182/339 [00:20<00:19,  7.90it/s, Materializing param=model.layers.14.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  54% 182/339 [00:20<00:19,  7.90it/s, Materializing param=model.layers.14.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  54% 183/339 [00:20<00:19,  7.90it/s, Materializing param=model.layers.15.input_layernorm.weight] \u001b[A\n",
            "Loading weights:  54% 183/339 [00:20<00:19,  7.90it/s, Materializing param=model.layers.15.input_layernorm.weight]\u001b[A\n",
            "Loading weights:  54% 184/339 [00:20<00:22,  6.91it/s, Materializing param=model.layers.15.input_layernorm.weight]\u001b[A\n",
            "Loading weights:  54% 184/339 [00:20<00:22,  6.91it/s, Materializing param=model.layers.15.mlp.down_proj.weight]  \u001b[A\n",
            "Loading weights:  54% 184/339 [00:20<00:22,  6.91it/s, Materializing param=model.layers.15.mlp.down_proj.weight]\u001b[A\n",
            "Loading weights:  55% 185/339 [00:20<00:22,  6.91it/s, Materializing param=model.layers.15.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:  55% 185/339 [00:20<00:22,  6.91it/s, Materializing param=model.layers.15.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:  55% 186/339 [00:21<00:26,  5.74it/s, Materializing param=model.layers.15.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:  55% 186/339 [00:21<00:26,  5.74it/s, Materializing param=model.layers.15.mlp.up_proj.weight]  \u001b[A\n",
            "Loading weights:  55% 186/339 [00:21<00:26,  5.74it/s, Materializing param=model.layers.15.mlp.up_proj.weight]\u001b[A\n",
            "Loading weights:  55% 187/339 [00:21<00:26,  5.74it/s, Materializing param=model.layers.15.post_attention_layernorm.weight]\u001b[A\n",
            "Loading weights:  55% 187/339 [00:21<00:26,  5.74it/s, Materializing param=model.layers.15.post_attention_layernorm.weight]\u001b[A\n",
            "Loading weights:  55% 188/339 [00:21<00:29,  5.06it/s, Materializing param=model.layers.15.post_attention_layernorm.weight]\u001b[A\n",
            "Loading weights:  55% 188/339 [00:21<00:29,  5.06it/s, Materializing param=model.layers.15.self_attn.k_proj.bias]          \u001b[A\n",
            "Loading weights:  55% 188/339 [00:21<00:29,  5.06it/s, Materializing param=model.layers.15.self_attn.k_proj.bias]\u001b[A\n",
            "Loading weights:  56% 189/339 [00:21<00:29,  5.06it/s, Materializing param=model.layers.15.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  56% 189/339 [00:21<00:29,  5.06it/s, Materializing param=model.layers.15.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  56% 190/339 [00:21<00:29,  5.06it/s, Materializing param=model.layers.15.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  56% 190/339 [00:21<00:29,  5.06it/s, Materializing param=model.layers.15.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  56% 191/339 [00:21<00:21,  7.00it/s, Materializing param=model.layers.15.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  56% 191/339 [00:21<00:21,  7.00it/s, Materializing param=model.layers.15.self_attn.q_proj.bias]  \u001b[A\n",
            "Loading weights:  56% 191/339 [00:21<00:21,  7.00it/s, Materializing param=model.layers.15.self_attn.q_proj.bias]\u001b[A\n",
            "Loading weights:  57% 192/339 [00:21<00:20,  7.00it/s, Materializing param=model.layers.15.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  57% 192/339 [00:21<00:20,  7.00it/s, Materializing param=model.layers.15.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  57% 193/339 [00:21<00:17,  8.31it/s, Materializing param=model.layers.15.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  57% 193/339 [00:21<00:17,  8.31it/s, Materializing param=model.layers.15.self_attn.v_proj.bias]  \u001b[A\n",
            "Loading weights:  57% 193/339 [00:21<00:17,  8.31it/s, Materializing param=model.layers.15.self_attn.v_proj.bias]\u001b[A\n",
            "Loading weights:  57% 194/339 [00:21<00:17,  8.31it/s, Materializing param=model.layers.15.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  57% 194/339 [00:21<00:17,  8.31it/s, Materializing param=model.layers.15.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  58% 195/339 [00:21<00:17,  8.31it/s, Materializing param=model.layers.16.input_layernorm.weight] \u001b[A\n",
            "Loading weights:  58% 195/339 [00:21<00:17,  8.31it/s, Materializing param=model.layers.16.input_layernorm.weight]\u001b[A\n",
            "Loading weights:  58% 196/339 [00:21<00:17,  8.31it/s, Materializing param=model.layers.16.mlp.down_proj.weight]  \u001b[A\n",
            "Loading weights:  58% 196/339 [00:21<00:17,  8.31it/s, Materializing param=model.layers.16.mlp.down_proj.weight]\u001b[A\n",
            "Loading weights:  58% 197/339 [00:22<00:18,  7.86it/s, Materializing param=model.layers.16.mlp.down_proj.weight]\u001b[A\n",
            "Loading weights:  58% 197/339 [00:22<00:18,  7.86it/s, Materializing param=model.layers.16.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:  58% 197/339 [00:22<00:18,  7.86it/s, Materializing param=model.layers.16.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:  58% 198/339 [00:23<00:17,  7.86it/s, Materializing param=model.layers.16.mlp.up_proj.weight]  \u001b[A\n",
            "Loading weights:  58% 198/339 [00:23<00:17,  7.86it/s, Materializing param=model.layers.16.mlp.up_proj.weight]\u001b[A\n",
            "Loading weights:  59% 199/339 [00:23<00:30,  4.52it/s, Materializing param=model.layers.16.mlp.up_proj.weight]\u001b[A\n",
            "Loading weights:  59% 199/339 [00:23<00:30,  4.52it/s, Materializing param=model.layers.16.post_attention_layernorm.weight]\u001b[A\n",
            "Loading weights:  59% 199/339 [00:23<00:30,  4.52it/s, Materializing param=model.layers.16.post_attention_layernorm.weight]\u001b[A\n",
            "Loading weights:  59% 200/339 [00:23<00:30,  4.52it/s, Materializing param=model.layers.16.self_attn.k_proj.bias]          \u001b[A\n",
            "Loading weights:  59% 200/339 [00:23<00:30,  4.52it/s, Materializing param=model.layers.16.self_attn.k_proj.bias]\u001b[A\n",
            "Loading weights:  59% 201/339 [00:23<00:30,  4.52it/s, Materializing param=model.layers.16.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  59% 201/339 [00:23<00:30,  4.52it/s, Materializing param=model.layers.16.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  60% 202/339 [00:23<00:30,  4.52it/s, Materializing param=model.layers.16.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  60% 202/339 [00:23<00:30,  4.52it/s, Materializing param=model.layers.16.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  60% 203/339 [00:23<00:19,  6.86it/s, Materializing param=model.layers.16.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  60% 203/339 [00:23<00:19,  6.86it/s, Materializing param=model.layers.16.self_attn.q_proj.bias]  \u001b[A\n",
            "Loading weights:  60% 203/339 [00:23<00:19,  6.86it/s, Materializing param=model.layers.16.self_attn.q_proj.bias]\u001b[A\n",
            "Loading weights:  60% 204/339 [00:23<00:19,  6.86it/s, Materializing param=model.layers.16.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  60% 204/339 [00:23<00:19,  6.86it/s, Materializing param=model.layers.16.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  60% 205/339 [00:23<00:19,  6.86it/s, Materializing param=model.layers.16.self_attn.v_proj.bias]  \u001b[A\n",
            "Loading weights:  60% 205/339 [00:23<00:19,  6.86it/s, Materializing param=model.layers.16.self_attn.v_proj.bias]\u001b[A\n",
            "Loading weights:  61% 206/339 [00:23<00:14,  8.88it/s, Materializing param=model.layers.16.self_attn.v_proj.bias]\u001b[A\n",
            "Loading weights:  61% 206/339 [00:23<00:14,  8.88it/s, Materializing param=model.layers.16.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  61% 206/339 [00:23<00:14,  8.88it/s, Materializing param=model.layers.16.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  61% 207/339 [00:23<00:14,  8.88it/s, Materializing param=model.layers.17.input_layernorm.weight] \u001b[A\n",
            "Loading weights:  61% 207/339 [00:23<00:14,  8.88it/s, Materializing param=model.layers.17.input_layernorm.weight]\u001b[A\n",
            "Loading weights:  61% 208/339 [00:23<00:14,  8.88it/s, Materializing param=model.layers.17.mlp.down_proj.weight]  \u001b[A\n",
            "Loading weights:  61% 208/339 [00:23<00:14,  8.88it/s, Materializing param=model.layers.17.mlp.down_proj.weight]\u001b[A\n",
            "Loading weights:  62% 209/339 [00:24<00:24,  5.32it/s, Materializing param=model.layers.17.mlp.down_proj.weight]\u001b[A\n",
            "Loading weights:  62% 209/339 [00:24<00:24,  5.32it/s, Materializing param=model.layers.17.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:  62% 209/339 [00:24<00:24,  5.32it/s, Materializing param=model.layers.17.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:  62% 210/339 [00:24<00:24,  5.32it/s, Materializing param=model.layers.17.mlp.up_proj.weight]  \u001b[A\n",
            "Loading weights:  62% 210/339 [00:24<00:24,  5.32it/s, Materializing param=model.layers.17.mlp.up_proj.weight]\u001b[A\n",
            "Loading weights:  62% 211/339 [00:25<00:26,  4.87it/s, Materializing param=model.layers.17.mlp.up_proj.weight]\u001b[A\n",
            "Loading weights:  62% 211/339 [00:25<00:26,  4.87it/s, Materializing param=model.layers.17.post_attention_layernorm.weight]\u001b[A\n",
            "Loading weights:  62% 211/339 [00:25<00:26,  4.87it/s, Materializing param=model.layers.17.post_attention_layernorm.weight]\u001b[A\n",
            "Loading weights:  63% 212/339 [00:25<00:26,  4.87it/s, Materializing param=model.layers.17.self_attn.k_proj.bias]          \u001b[A\n",
            "Loading weights:  63% 212/339 [00:25<00:26,  4.87it/s, Materializing param=model.layers.17.self_attn.k_proj.bias]\u001b[A\n",
            "Loading weights:  63% 213/339 [00:25<00:22,  5.49it/s, Materializing param=model.layers.17.self_attn.k_proj.bias]\u001b[A\n",
            "Loading weights:  63% 213/339 [00:25<00:22,  5.49it/s, Materializing param=model.layers.17.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  63% 213/339 [00:25<00:22,  5.49it/s, Materializing param=model.layers.17.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  63% 214/339 [00:25<00:22,  5.49it/s, Materializing param=model.layers.17.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  63% 214/339 [00:25<00:22,  5.49it/s, Materializing param=model.layers.17.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  63% 215/339 [00:25<00:22,  5.49it/s, Materializing param=model.layers.17.self_attn.q_proj.bias]  \u001b[A\n",
            "Loading weights:  63% 215/339 [00:25<00:22,  5.49it/s, Materializing param=model.layers.17.self_attn.q_proj.bias]\u001b[A\n",
            "Loading weights:  64% 216/339 [00:25<00:22,  5.49it/s, Materializing param=model.layers.17.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  64% 216/339 [00:25<00:22,  5.49it/s, Materializing param=model.layers.17.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  64% 217/339 [00:25<00:22,  5.49it/s, Materializing param=model.layers.17.self_attn.v_proj.bias]  \u001b[A\n",
            "Loading weights:  64% 217/339 [00:25<00:22,  5.49it/s, Materializing param=model.layers.17.self_attn.v_proj.bias]\u001b[A\n",
            "Loading weights:  64% 218/339 [00:25<00:22,  5.49it/s, Materializing param=model.layers.17.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  64% 218/339 [00:25<00:22,  5.49it/s, Materializing param=model.layers.17.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  65% 219/339 [00:25<00:21,  5.49it/s, Materializing param=model.layers.18.input_layernorm.weight] \u001b[A\n",
            "Loading weights:  65% 219/339 [00:25<00:21,  5.49it/s, Materializing param=model.layers.18.input_layernorm.weight]\u001b[A\n",
            "Loading weights:  65% 220/339 [00:26<00:15,  7.92it/s, Materializing param=model.layers.18.input_layernorm.weight]\u001b[A\n",
            "Loading weights:  65% 220/339 [00:26<00:15,  7.92it/s, Materializing param=model.layers.18.mlp.down_proj.weight]  \u001b[A\n",
            "Loading weights:  65% 220/339 [00:26<00:15,  7.92it/s, Materializing param=model.layers.18.mlp.down_proj.weight]\u001b[A\n",
            "Loading weights:  65% 221/339 [00:26<00:14,  7.92it/s, Materializing param=model.layers.18.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:  65% 221/339 [00:26<00:14,  7.92it/s, Materializing param=model.layers.18.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:  65% 222/339 [00:26<00:17,  6.55it/s, Materializing param=model.layers.18.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:  65% 222/339 [00:26<00:17,  6.55it/s, Materializing param=model.layers.18.mlp.up_proj.weight]  \u001b[A\n",
            "Loading weights:  65% 222/339 [00:26<00:17,  6.55it/s, Materializing param=model.layers.18.mlp.up_proj.weight]\u001b[A\n",
            "Loading weights:  66% 223/339 [00:27<00:22,  5.07it/s, Materializing param=model.layers.18.mlp.up_proj.weight]\u001b[A\n",
            "Loading weights:  66% 223/339 [00:27<00:22,  5.07it/s, Materializing param=model.layers.18.post_attention_layernorm.weight]\u001b[A\n",
            "Loading weights:  66% 223/339 [00:27<00:22,  5.07it/s, Materializing param=model.layers.18.post_attention_layernorm.weight]\u001b[A\n",
            "Loading weights:  66% 224/339 [00:27<00:22,  5.07it/s, Materializing param=model.layers.18.self_attn.k_proj.bias]          \u001b[A\n",
            "Loading weights:  66% 224/339 [00:27<00:22,  5.07it/s, Materializing param=model.layers.18.self_attn.k_proj.bias]\u001b[A\n",
            "Loading weights:  66% 225/339 [00:27<00:22,  5.07it/s, Materializing param=model.layers.18.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  66% 225/339 [00:27<00:22,  5.07it/s, Materializing param=model.layers.18.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  67% 226/339 [00:27<00:22,  5.07it/s, Materializing param=model.layers.18.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  67% 226/339 [00:27<00:22,  5.07it/s, Materializing param=model.layers.18.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  67% 227/339 [00:27<00:14,  7.64it/s, Materializing param=model.layers.18.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  67% 227/339 [00:27<00:14,  7.64it/s, Materializing param=model.layers.18.self_attn.q_proj.bias]  \u001b[A\n",
            "Loading weights:  67% 227/339 [00:27<00:14,  7.64it/s, Materializing param=model.layers.18.self_attn.q_proj.bias]\u001b[A\n",
            "Loading weights:  67% 228/339 [00:27<00:14,  7.64it/s, Materializing param=model.layers.18.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  67% 228/339 [00:27<00:14,  7.64it/s, Materializing param=model.layers.18.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  68% 229/339 [00:27<00:12,  8.85it/s, Materializing param=model.layers.18.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  68% 229/339 [00:27<00:12,  8.85it/s, Materializing param=model.layers.18.self_attn.v_proj.bias]  \u001b[A\n",
            "Loading weights:  68% 229/339 [00:27<00:12,  8.85it/s, Materializing param=model.layers.18.self_attn.v_proj.bias]\u001b[A\n",
            "Loading weights:  68% 230/339 [00:27<00:12,  8.85it/s, Materializing param=model.layers.18.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  68% 230/339 [00:27<00:12,  8.85it/s, Materializing param=model.layers.18.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  68% 231/339 [00:27<00:12,  8.85it/s, Materializing param=model.layers.19.input_layernorm.weight] \u001b[A\n",
            "Loading weights:  68% 231/339 [00:27<00:12,  8.85it/s, Materializing param=model.layers.19.input_layernorm.weight]\u001b[A\n",
            "Loading weights:  68% 232/339 [00:27<00:12,  8.85it/s, Materializing param=model.layers.19.mlp.down_proj.weight]  \u001b[A\n",
            "Loading weights:  68% 232/339 [00:27<00:12,  8.85it/s, Materializing param=model.layers.19.mlp.down_proj.weight]\u001b[A\n",
            "Loading weights:  69% 233/339 [00:28<00:13,  8.12it/s, Materializing param=model.layers.19.mlp.down_proj.weight]\u001b[A\n",
            "Loading weights:  69% 233/339 [00:28<00:13,  8.12it/s, Materializing param=model.layers.19.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:  69% 233/339 [00:28<00:13,  8.12it/s, Materializing param=model.layers.19.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:  69% 234/339 [00:28<00:12,  8.12it/s, Materializing param=model.layers.19.mlp.up_proj.weight]  \u001b[A\n",
            "Loading weights:  69% 234/339 [00:28<00:12,  8.12it/s, Materializing param=model.layers.19.mlp.up_proj.weight]\u001b[A\n",
            "Loading weights:  69% 235/339 [00:29<00:22,  4.68it/s, Materializing param=model.layers.19.mlp.up_proj.weight]\u001b[A\n",
            "Loading weights:  69% 235/339 [00:29<00:22,  4.68it/s, Materializing param=model.layers.19.post_attention_layernorm.weight]\u001b[A\n",
            "Loading weights:  69% 235/339 [00:29<00:22,  4.68it/s, Materializing param=model.layers.19.post_attention_layernorm.weight]\u001b[A\n",
            "Loading weights:  70% 236/339 [00:29<00:21,  4.68it/s, Materializing param=model.layers.19.self_attn.k_proj.bias]          \u001b[A\n",
            "Loading weights:  70% 236/339 [00:29<00:21,  4.68it/s, Materializing param=model.layers.19.self_attn.k_proj.bias]\u001b[A\n",
            "Loading weights:  70% 237/339 [00:29<00:21,  4.68it/s, Materializing param=model.layers.19.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  70% 237/339 [00:29<00:21,  4.68it/s, Materializing param=model.layers.19.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  70% 238/339 [00:29<00:21,  4.68it/s, Materializing param=model.layers.19.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  70% 238/339 [00:29<00:21,  4.68it/s, Materializing param=model.layers.19.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  71% 239/339 [00:29<00:14,  6.98it/s, Materializing param=model.layers.19.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  71% 239/339 [00:29<00:14,  6.98it/s, Materializing param=model.layers.19.self_attn.q_proj.bias]  \u001b[A\n",
            "Loading weights:  71% 239/339 [00:29<00:14,  6.98it/s, Materializing param=model.layers.19.self_attn.q_proj.bias]\u001b[A\n",
            "Loading weights:  71% 240/339 [00:29<00:14,  6.98it/s, Materializing param=model.layers.19.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  71% 240/339 [00:29<00:14,  6.98it/s, Materializing param=model.layers.19.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  71% 241/339 [00:29<00:12,  8.06it/s, Materializing param=model.layers.19.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  71% 241/339 [00:29<00:12,  8.06it/s, Materializing param=model.layers.19.self_attn.v_proj.bias]  \u001b[A\n",
            "Loading weights:  71% 241/339 [00:29<00:12,  8.06it/s, Materializing param=model.layers.19.self_attn.v_proj.bias]\u001b[A\n",
            "Loading weights:  71% 242/339 [00:29<00:12,  8.06it/s, Materializing param=model.layers.19.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  71% 242/339 [00:29<00:12,  8.06it/s, Materializing param=model.layers.19.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  72% 243/339 [00:29<00:11,  8.06it/s, Materializing param=model.layers.20.input_layernorm.weight] \u001b[A\n",
            "Loading weights:  72% 243/339 [00:29<00:11,  8.06it/s, Materializing param=model.layers.20.input_layernorm.weight]\u001b[A\n",
            "Loading weights:  72% 244/339 [00:29<00:13,  6.98it/s, Materializing param=model.layers.20.input_layernorm.weight]\u001b[A\n",
            "Loading weights:  72% 244/339 [00:29<00:13,  6.98it/s, Materializing param=model.layers.20.mlp.down_proj.weight]  \u001b[A\n",
            "Loading weights:  72% 244/339 [00:29<00:13,  6.98it/s, Materializing param=model.layers.20.mlp.down_proj.weight]\u001b[A\n",
            "Loading weights:  72% 245/339 [00:29<00:13,  6.98it/s, Materializing param=model.layers.20.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:  72% 245/339 [00:29<00:13,  6.98it/s, Materializing param=model.layers.20.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:  73% 246/339 [00:30<00:16,  5.76it/s, Materializing param=model.layers.20.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:  73% 246/339 [00:30<00:16,  5.76it/s, Materializing param=model.layers.20.mlp.up_proj.weight]  \u001b[A\n",
            "Loading weights:  73% 246/339 [00:30<00:16,  5.76it/s, Materializing param=model.layers.20.mlp.up_proj.weight]\u001b[A\n",
            "Loading weights:  73% 247/339 [00:31<00:15,  5.76it/s, Materializing param=model.layers.20.post_attention_layernorm.weight]\u001b[A\n",
            "Loading weights:  73% 247/339 [00:31<00:15,  5.76it/s, Materializing param=model.layers.20.post_attention_layernorm.weight]\u001b[A\n",
            "Loading weights:  73% 248/339 [00:31<00:18,  5.05it/s, Materializing param=model.layers.20.post_attention_layernorm.weight]\u001b[A\n",
            "Loading weights:  73% 248/339 [00:31<00:18,  5.05it/s, Materializing param=model.layers.20.self_attn.k_proj.bias]          \u001b[A\n",
            "Loading weights:  73% 248/339 [00:31<00:18,  5.05it/s, Materializing param=model.layers.20.self_attn.k_proj.bias]\u001b[A\n",
            "Loading weights:  73% 249/339 [00:31<00:17,  5.05it/s, Materializing param=model.layers.20.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  73% 249/339 [00:31<00:17,  5.05it/s, Materializing param=model.layers.20.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  74% 250/339 [00:31<00:17,  5.05it/s, Materializing param=model.layers.20.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  74% 250/339 [00:31<00:17,  5.05it/s, Materializing param=model.layers.20.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  74% 251/339 [00:31<00:12,  6.99it/s, Materializing param=model.layers.20.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  74% 251/339 [00:31<00:12,  6.99it/s, Materializing param=model.layers.20.self_attn.q_proj.bias]  \u001b[A\n",
            "Loading weights:  74% 251/339 [00:31<00:12,  6.99it/s, Materializing param=model.layers.20.self_attn.q_proj.bias]\u001b[A\n",
            "Loading weights:  74% 252/339 [00:31<00:12,  6.99it/s, Materializing param=model.layers.20.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  74% 252/339 [00:31<00:12,  6.99it/s, Materializing param=model.layers.20.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  75% 253/339 [00:31<00:10,  8.23it/s, Materializing param=model.layers.20.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  75% 253/339 [00:31<00:10,  8.23it/s, Materializing param=model.layers.20.self_attn.v_proj.bias]  \u001b[A\n",
            "Loading weights:  75% 253/339 [00:31<00:10,  8.23it/s, Materializing param=model.layers.20.self_attn.v_proj.bias]\u001b[A\n",
            "Loading weights:  75% 254/339 [00:31<00:10,  8.23it/s, Materializing param=model.layers.20.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  75% 254/339 [00:31<00:10,  8.23it/s, Materializing param=model.layers.20.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  75% 255/339 [00:31<00:10,  8.23it/s, Materializing param=model.layers.21.input_layernorm.weight] \u001b[A\n",
            "Loading weights:  75% 255/339 [00:31<00:10,  8.23it/s, Materializing param=model.layers.21.input_layernorm.weight]\u001b[A\n",
            "Loading weights:  76% 256/339 [00:31<00:10,  8.23it/s, Materializing param=model.layers.21.mlp.down_proj.weight]  \u001b[A\n",
            "Loading weights:  76% 256/339 [00:31<00:10,  8.23it/s, Materializing param=model.layers.21.mlp.down_proj.weight]\u001b[A\n",
            "Loading weights:  76% 257/339 [00:31<00:10,  7.76it/s, Materializing param=model.layers.21.mlp.down_proj.weight]\u001b[A\n",
            "Loading weights:  76% 257/339 [00:31<00:10,  7.76it/s, Materializing param=model.layers.21.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:  76% 257/339 [00:31<00:10,  7.76it/s, Materializing param=model.layers.21.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:  76% 258/339 [00:32<00:10,  7.76it/s, Materializing param=model.layers.21.mlp.up_proj.weight]  \u001b[A\n",
            "Loading weights:  76% 258/339 [00:32<00:10,  7.76it/s, Materializing param=model.layers.21.mlp.up_proj.weight]\u001b[A\n",
            "Loading weights:  76% 259/339 [00:32<00:17,  4.48it/s, Materializing param=model.layers.21.mlp.up_proj.weight]\u001b[A\n",
            "Loading weights:  76% 259/339 [00:32<00:17,  4.48it/s, Materializing param=model.layers.21.post_attention_layernorm.weight]\u001b[A\n",
            "Loading weights:  76% 259/339 [00:32<00:17,  4.48it/s, Materializing param=model.layers.21.post_attention_layernorm.weight]\u001b[A\n",
            "Loading weights:  77% 260/339 [00:32<00:17,  4.48it/s, Materializing param=model.layers.21.self_attn.k_proj.bias]          \u001b[A\n",
            "Loading weights:  77% 260/339 [00:32<00:17,  4.48it/s, Materializing param=model.layers.21.self_attn.k_proj.bias]\u001b[A\n",
            "Loading weights:  77% 261/339 [00:32<00:17,  4.48it/s, Materializing param=model.layers.21.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  77% 261/339 [00:32<00:17,  4.48it/s, Materializing param=model.layers.21.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  77% 262/339 [00:32<00:17,  4.48it/s, Materializing param=model.layers.21.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  77% 262/339 [00:32<00:17,  4.48it/s, Materializing param=model.layers.21.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  78% 263/339 [00:33<00:11,  6.81it/s, Materializing param=model.layers.21.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  78% 263/339 [00:33<00:11,  6.81it/s, Materializing param=model.layers.21.self_attn.q_proj.bias]  \u001b[A\n",
            "Loading weights:  78% 263/339 [00:33<00:11,  6.81it/s, Materializing param=model.layers.21.self_attn.q_proj.bias]\u001b[A\n",
            "Loading weights:  78% 264/339 [00:33<00:11,  6.81it/s, Materializing param=model.layers.21.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  78% 264/339 [00:33<00:11,  6.81it/s, Materializing param=model.layers.21.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  78% 265/339 [00:33<00:09,  7.93it/s, Materializing param=model.layers.21.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  78% 265/339 [00:33<00:09,  7.93it/s, Materializing param=model.layers.21.self_attn.v_proj.bias]  \u001b[A\n",
            "Loading weights:  78% 265/339 [00:33<00:09,  7.93it/s, Materializing param=model.layers.21.self_attn.v_proj.bias]\u001b[A\n",
            "Loading weights:  78% 266/339 [00:33<00:09,  7.93it/s, Materializing param=model.layers.21.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  78% 266/339 [00:33<00:09,  7.93it/s, Materializing param=model.layers.21.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  79% 267/339 [00:33<00:09,  7.93it/s, Materializing param=model.layers.22.input_layernorm.weight] \u001b[A\n",
            "Loading weights:  79% 267/339 [00:33<00:09,  7.93it/s, Materializing param=model.layers.22.input_layernorm.weight]\u001b[A\n",
            "Loading weights:  79% 268/339 [00:33<00:08,  7.93it/s, Materializing param=model.layers.22.mlp.down_proj.weight]  \u001b[A\n",
            "Loading weights:  79% 268/339 [00:33<00:08,  7.93it/s, Materializing param=model.layers.22.mlp.down_proj.weight]\u001b[A\n",
            "Loading weights:  79% 269/339 [00:34<00:12,  5.43it/s, Materializing param=model.layers.22.mlp.down_proj.weight]\u001b[A\n",
            "Loading weights:  79% 269/339 [00:34<00:12,  5.43it/s, Materializing param=model.layers.22.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:  79% 269/339 [00:34<00:12,  5.43it/s, Materializing param=model.layers.22.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:  80% 270/339 [00:34<00:12,  5.43it/s, Materializing param=model.layers.22.mlp.up_proj.weight]  \u001b[A\n",
            "Loading weights:  80% 270/339 [00:34<00:12,  5.43it/s, Materializing param=model.layers.22.mlp.up_proj.weight]\u001b[A\n",
            "Loading weights:  80% 271/339 [00:34<00:13,  4.93it/s, Materializing param=model.layers.22.mlp.up_proj.weight]\u001b[A\n",
            "Loading weights:  80% 271/339 [00:34<00:13,  4.93it/s, Materializing param=model.layers.22.post_attention_layernorm.weight]\u001b[A\n",
            "Loading weights:  80% 271/339 [00:34<00:13,  4.93it/s, Materializing param=model.layers.22.post_attention_layernorm.weight]\u001b[A\n",
            "Loading weights:  80% 272/339 [00:34<00:13,  4.93it/s, Materializing param=model.layers.22.self_attn.k_proj.bias]          \u001b[A\n",
            "Loading weights:  80% 272/339 [00:34<00:13,  4.93it/s, Materializing param=model.layers.22.self_attn.k_proj.bias]\u001b[A\n",
            "Loading weights:  81% 273/339 [00:34<00:13,  4.93it/s, Materializing param=model.layers.22.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  81% 273/339 [00:34<00:13,  4.93it/s, Materializing param=model.layers.22.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  81% 274/339 [00:34<00:13,  4.93it/s, Materializing param=model.layers.22.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  81% 274/339 [00:34<00:13,  4.93it/s, Materializing param=model.layers.22.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  81% 275/339 [00:34<00:08,  7.32it/s, Materializing param=model.layers.22.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  81% 275/339 [00:34<00:08,  7.32it/s, Materializing param=model.layers.22.self_attn.q_proj.bias]  \u001b[A\n",
            "Loading weights:  81% 275/339 [00:34<00:08,  7.32it/s, Materializing param=model.layers.22.self_attn.q_proj.bias]\u001b[A\n",
            "Loading weights:  81% 276/339 [00:34<00:08,  7.32it/s, Materializing param=model.layers.22.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  81% 276/339 [00:34<00:08,  7.32it/s, Materializing param=model.layers.22.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  82% 277/339 [00:35<00:07,  8.31it/s, Materializing param=model.layers.22.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  82% 277/339 [00:35<00:07,  8.31it/s, Materializing param=model.layers.22.self_attn.v_proj.bias]  \u001b[A\n",
            "Loading weights:  82% 277/339 [00:35<00:07,  8.31it/s, Materializing param=model.layers.22.self_attn.v_proj.bias]\u001b[A\n",
            "Loading weights:  82% 278/339 [00:35<00:07,  8.31it/s, Materializing param=model.layers.22.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  82% 278/339 [00:35<00:07,  8.31it/s, Materializing param=model.layers.22.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  82% 279/339 [00:35<00:07,  8.31it/s, Materializing param=model.layers.23.input_layernorm.weight] \u001b[A\n",
            "Loading weights:  82% 279/339 [00:35<00:07,  8.31it/s, Materializing param=model.layers.23.input_layernorm.weight]\u001b[A\n",
            "Loading weights:  83% 280/339 [00:35<00:07,  8.31it/s, Materializing param=model.layers.23.mlp.down_proj.weight]  \u001b[A\n",
            "Loading weights:  83% 280/339 [00:35<00:07,  8.31it/s, Materializing param=model.layers.23.mlp.down_proj.weight]\u001b[A\n",
            "Loading weights:  83% 281/339 [00:36<00:10,  5.63it/s, Materializing param=model.layers.23.mlp.down_proj.weight]\u001b[A\n",
            "Loading weights:  83% 281/339 [00:36<00:10,  5.63it/s, Materializing param=model.layers.23.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:  83% 281/339 [00:36<00:10,  5.63it/s, Materializing param=model.layers.23.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:  83% 282/339 [00:36<00:10,  5.63it/s, Materializing param=model.layers.23.mlp.up_proj.weight]  \u001b[A\n",
            "Loading weights:  83% 282/339 [00:36<00:10,  5.63it/s, Materializing param=model.layers.23.mlp.up_proj.weight]\u001b[A\n",
            "Loading weights:  83% 283/339 [00:36<00:11,  5.08it/s, Materializing param=model.layers.23.mlp.up_proj.weight]\u001b[A\n",
            "Loading weights:  83% 283/339 [00:36<00:11,  5.08it/s, Materializing param=model.layers.23.post_attention_layernorm.weight]\u001b[A\n",
            "Loading weights:  83% 283/339 [00:36<00:11,  5.08it/s, Materializing param=model.layers.23.post_attention_layernorm.weight]\u001b[A\n",
            "Loading weights:  84% 284/339 [00:36<00:10,  5.08it/s, Materializing param=model.layers.23.self_attn.k_proj.bias]          \u001b[A\n",
            "Loading weights:  84% 284/339 [00:36<00:10,  5.08it/s, Materializing param=model.layers.23.self_attn.k_proj.bias]\u001b[A\n",
            "Loading weights:  84% 285/339 [00:36<00:10,  5.08it/s, Materializing param=model.layers.23.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  84% 285/339 [00:36<00:10,  5.08it/s, Materializing param=model.layers.23.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  84% 286/339 [00:36<00:10,  5.08it/s, Materializing param=model.layers.23.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  84% 286/339 [00:36<00:10,  5.08it/s, Materializing param=model.layers.23.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  85% 287/339 [00:36<00:06,  7.48it/s, Materializing param=model.layers.23.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  85% 287/339 [00:36<00:06,  7.48it/s, Materializing param=model.layers.23.self_attn.q_proj.bias]  \u001b[A\n",
            "Loading weights:  85% 287/339 [00:36<00:06,  7.48it/s, Materializing param=model.layers.23.self_attn.q_proj.bias]\u001b[A\n",
            "Loading weights:  85% 288/339 [00:36<00:06,  7.48it/s, Materializing param=model.layers.23.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  85% 288/339 [00:36<00:06,  7.48it/s, Materializing param=model.layers.23.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  85% 289/339 [00:36<00:05,  8.60it/s, Materializing param=model.layers.23.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  85% 289/339 [00:36<00:05,  8.60it/s, Materializing param=model.layers.23.self_attn.v_proj.bias]  \u001b[A\n",
            "Loading weights:  85% 289/339 [00:36<00:05,  8.60it/s, Materializing param=model.layers.23.self_attn.v_proj.bias]\u001b[A\n",
            "Loading weights:  86% 290/339 [00:36<00:05,  8.60it/s, Materializing param=model.layers.23.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  86% 290/339 [00:36<00:05,  8.60it/s, Materializing param=model.layers.23.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  86% 291/339 [00:36<00:05,  8.60it/s, Materializing param=model.layers.24.input_layernorm.weight] \u001b[A\n",
            "Loading weights:  86% 291/339 [00:36<00:05,  8.60it/s, Materializing param=model.layers.24.input_layernorm.weight]\u001b[A\n",
            "Loading weights:  86% 292/339 [00:36<00:05,  8.60it/s, Materializing param=model.layers.24.mlp.down_proj.weight]  \u001b[A\n",
            "Loading weights:  86% 292/339 [00:36<00:05,  8.60it/s, Materializing param=model.layers.24.mlp.down_proj.weight]\u001b[A\n",
            "Loading weights:  86% 293/339 [00:37<00:05,  8.01it/s, Materializing param=model.layers.24.mlp.down_proj.weight]\u001b[A\n",
            "Loading weights:  86% 293/339 [00:37<00:05,  8.01it/s, Materializing param=model.layers.24.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:  86% 293/339 [00:37<00:05,  8.01it/s, Materializing param=model.layers.24.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:  87% 294/339 [00:37<00:05,  8.01it/s, Materializing param=model.layers.24.mlp.up_proj.weight]  \u001b[A\n",
            "Loading weights:  87% 294/339 [00:37<00:05,  8.01it/s, Materializing param=model.layers.24.mlp.up_proj.weight]\u001b[A\n",
            "Loading weights:  87% 295/339 [00:38<00:09,  4.68it/s, Materializing param=model.layers.24.mlp.up_proj.weight]\u001b[A\n",
            "Loading weights:  87% 295/339 [00:38<00:09,  4.68it/s, Materializing param=model.layers.24.post_attention_layernorm.weight]\u001b[A\n",
            "Loading weights:  87% 295/339 [00:38<00:09,  4.68it/s, Materializing param=model.layers.24.post_attention_layernorm.weight]\u001b[A\n",
            "Loading weights:  87% 296/339 [00:38<00:09,  4.68it/s, Materializing param=model.layers.24.self_attn.k_proj.bias]          \u001b[A\n",
            "Loading weights:  87% 296/339 [00:38<00:09,  4.68it/s, Materializing param=model.layers.24.self_attn.k_proj.bias]\u001b[A\n",
            "Loading weights:  88% 297/339 [00:38<00:08,  4.68it/s, Materializing param=model.layers.24.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  88% 297/339 [00:38<00:08,  4.68it/s, Materializing param=model.layers.24.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  88% 298/339 [00:38<00:08,  4.68it/s, Materializing param=model.layers.24.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  88% 298/339 [00:38<00:08,  4.68it/s, Materializing param=model.layers.24.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  88% 299/339 [00:38<00:05,  6.95it/s, Materializing param=model.layers.24.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  88% 299/339 [00:38<00:05,  6.95it/s, Materializing param=model.layers.24.self_attn.q_proj.bias]  \u001b[A\n",
            "Loading weights:  88% 299/339 [00:38<00:05,  6.95it/s, Materializing param=model.layers.24.self_attn.q_proj.bias]\u001b[A\n",
            "Loading weights:  88% 300/339 [00:38<00:05,  6.95it/s, Materializing param=model.layers.24.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  88% 300/339 [00:38<00:05,  6.95it/s, Materializing param=model.layers.24.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  89% 301/339 [00:38<00:04,  8.03it/s, Materializing param=model.layers.24.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  89% 301/339 [00:38<00:04,  8.03it/s, Materializing param=model.layers.24.self_attn.v_proj.bias]  \u001b[A\n",
            "Loading weights:  89% 301/339 [00:38<00:04,  8.03it/s, Materializing param=model.layers.24.self_attn.v_proj.bias]\u001b[A\n",
            "Loading weights:  89% 302/339 [00:38<00:04,  8.03it/s, Materializing param=model.layers.24.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  89% 302/339 [00:38<00:04,  8.03it/s, Materializing param=model.layers.24.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  89% 303/339 [00:38<00:04,  8.03it/s, Materializing param=model.layers.25.input_layernorm.weight] \u001b[A\n",
            "Loading weights:  89% 303/339 [00:38<00:04,  8.03it/s, Materializing param=model.layers.25.input_layernorm.weight]\u001b[A\n",
            "Loading weights:  90% 304/339 [00:38<00:04,  8.03it/s, Materializing param=model.layers.25.mlp.down_proj.weight]  \u001b[A\n",
            "Loading weights:  90% 304/339 [00:38<00:04,  8.03it/s, Materializing param=model.layers.25.mlp.down_proj.weight]\u001b[A\n",
            "Loading weights:  90% 305/339 [00:38<00:04,  8.03it/s, Materializing param=model.layers.25.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:  90% 305/339 [00:38<00:04,  8.03it/s, Materializing param=model.layers.25.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:  90% 306/339 [00:38<00:04,  8.03it/s, Materializing param=model.layers.25.mlp.up_proj.weight]  \u001b[A\n",
            "Loading weights:  90% 306/339 [00:38<00:04,  8.03it/s, Materializing param=model.layers.25.mlp.up_proj.weight]\u001b[A\n",
            "Loading weights:  91% 307/339 [00:38<00:03,  8.03it/s, Materializing param=model.layers.25.post_attention_layernorm.weight]\u001b[A\n",
            "Loading weights:  91% 307/339 [00:38<00:03,  8.03it/s, Materializing param=model.layers.25.post_attention_layernorm.weight]\u001b[A\n",
            "Loading weights:  91% 308/339 [00:38<00:03,  8.03it/s, Materializing param=model.layers.25.self_attn.k_proj.bias]          \u001b[A\n",
            "Loading weights:  91% 308/339 [00:38<00:03,  8.03it/s, Materializing param=model.layers.25.self_attn.k_proj.bias]\u001b[A\n",
            "Loading weights:  91% 309/339 [00:38<00:03,  8.03it/s, Materializing param=model.layers.25.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  91% 309/339 [00:38<00:03,  8.03it/s, Materializing param=model.layers.25.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  91% 310/339 [00:38<00:03,  8.03it/s, Materializing param=model.layers.25.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  91% 310/339 [00:38<00:03,  8.03it/s, Materializing param=model.layers.25.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  92% 311/339 [00:38<00:03,  8.03it/s, Materializing param=model.layers.25.self_attn.q_proj.bias]  \u001b[A\n",
            "Loading weights:  92% 311/339 [00:38<00:03,  8.03it/s, Materializing param=model.layers.25.self_attn.q_proj.bias]\u001b[A\n",
            "Loading weights:  92% 312/339 [00:38<00:03,  8.03it/s, Materializing param=model.layers.25.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  92% 312/339 [00:38<00:03,  8.03it/s, Materializing param=model.layers.25.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  92% 313/339 [00:38<00:03,  8.03it/s, Materializing param=model.layers.25.self_attn.v_proj.bias]  \u001b[A\n",
            "Loading weights:  92% 313/339 [00:38<00:03,  8.03it/s, Materializing param=model.layers.25.self_attn.v_proj.bias]\u001b[A\n",
            "Loading weights:  93% 314/339 [00:38<00:03,  8.03it/s, Materializing param=model.layers.25.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  93% 314/339 [00:38<00:03,  8.03it/s, Materializing param=model.layers.25.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  93% 315/339 [00:38<00:02,  8.03it/s, Materializing param=model.layers.26.input_layernorm.weight] \u001b[A\n",
            "Loading weights:  93% 315/339 [00:38<00:02,  8.03it/s, Materializing param=model.layers.26.input_layernorm.weight]\u001b[A\n",
            "Loading weights:  93% 316/339 [00:38<00:02,  8.03it/s, Materializing param=model.layers.26.mlp.down_proj.weight]  \u001b[A\n",
            "Loading weights:  93% 316/339 [00:38<00:02,  8.03it/s, Materializing param=model.layers.26.mlp.down_proj.weight]\u001b[A\n",
            "Loading weights:  94% 317/339 [00:38<00:02,  8.03it/s, Materializing param=model.layers.26.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:  94% 317/339 [00:38<00:02,  8.03it/s, Materializing param=model.layers.26.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:  94% 318/339 [00:38<00:02,  8.03it/s, Materializing param=model.layers.26.mlp.up_proj.weight]  \u001b[A\n",
            "Loading weights:  94% 318/339 [00:38<00:02,  8.03it/s, Materializing param=model.layers.26.mlp.up_proj.weight]\u001b[A\n",
            "Loading weights:  94% 319/339 [00:38<00:02,  8.03it/s, Materializing param=model.layers.26.post_attention_layernorm.weight]\u001b[A\n",
            "Loading weights:  94% 319/339 [00:38<00:02,  8.03it/s, Materializing param=model.layers.26.post_attention_layernorm.weight]\u001b[A\n",
            "Loading weights:  94% 320/339 [00:38<00:02,  8.03it/s, Materializing param=model.layers.26.self_attn.k_proj.bias]          \u001b[A\n",
            "Loading weights:  94% 320/339 [00:38<00:02,  8.03it/s, Materializing param=model.layers.26.self_attn.k_proj.bias]\u001b[A\n",
            "Loading weights:  95% 321/339 [00:38<00:02,  8.03it/s, Materializing param=model.layers.26.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  95% 321/339 [00:38<00:02,  8.03it/s, Materializing param=model.layers.26.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  95% 322/339 [00:38<00:02,  8.03it/s, Materializing param=model.layers.26.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  95% 322/339 [00:38<00:02,  8.03it/s, Materializing param=model.layers.26.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  95% 323/339 [00:38<00:01,  8.03it/s, Materializing param=model.layers.26.self_attn.q_proj.bias]  \u001b[A\n",
            "Loading weights:  95% 323/339 [00:38<00:01,  8.03it/s, Materializing param=model.layers.26.self_attn.q_proj.bias]\u001b[A\n",
            "Loading weights:  96% 324/339 [00:38<00:01,  8.03it/s, Materializing param=model.layers.26.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  96% 324/339 [00:38<00:01,  8.03it/s, Materializing param=model.layers.26.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  96% 325/339 [00:38<00:01,  8.03it/s, Materializing param=model.layers.26.self_attn.v_proj.bias]  \u001b[A\n",
            "Loading weights:  96% 325/339 [00:38<00:01,  8.03it/s, Materializing param=model.layers.26.self_attn.v_proj.bias]\u001b[A\n",
            "Loading weights:  96% 326/339 [00:38<00:01,  8.03it/s, Materializing param=model.layers.26.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  96% 326/339 [00:38<00:01,  8.03it/s, Materializing param=model.layers.26.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights:  96% 327/339 [00:38<00:01,  8.03it/s, Materializing param=model.layers.27.input_layernorm.weight] \u001b[A\n",
            "Loading weights:  96% 327/339 [00:38<00:01,  8.03it/s, Materializing param=model.layers.27.input_layernorm.weight]\u001b[A\n",
            "Loading weights:  97% 328/339 [00:38<00:01,  8.03it/s, Materializing param=model.layers.27.mlp.down_proj.weight]  \u001b[A\n",
            "Loading weights:  97% 328/339 [00:38<00:01,  8.03it/s, Materializing param=model.layers.27.mlp.down_proj.weight]\u001b[A\n",
            "Loading weights:  97% 329/339 [00:38<00:01,  8.03it/s, Materializing param=model.layers.27.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:  97% 329/339 [00:38<00:01,  8.03it/s, Materializing param=model.layers.27.mlp.gate_proj.weight]\u001b[A\n",
            "Loading weights:  97% 330/339 [00:38<00:01,  8.03it/s, Materializing param=model.layers.27.mlp.up_proj.weight]  \u001b[A\n",
            "Loading weights:  97% 330/339 [00:38<00:01,  8.03it/s, Materializing param=model.layers.27.mlp.up_proj.weight]\u001b[A\n",
            "Loading weights:  98% 331/339 [00:38<00:00,  8.03it/s, Materializing param=model.layers.27.post_attention_layernorm.weight]\u001b[A\n",
            "Loading weights:  98% 331/339 [00:38<00:00,  8.03it/s, Materializing param=model.layers.27.post_attention_layernorm.weight]\u001b[A\n",
            "Loading weights:  98% 332/339 [00:38<00:00,  8.03it/s, Materializing param=model.layers.27.self_attn.k_proj.bias]          \u001b[A\n",
            "Loading weights:  98% 332/339 [00:38<00:00,  8.03it/s, Materializing param=model.layers.27.self_attn.k_proj.bias]\u001b[A\n",
            "Loading weights:  98% 333/339 [00:38<00:00,  8.03it/s, Materializing param=model.layers.27.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  98% 333/339 [00:38<00:00,  8.03it/s, Materializing param=model.layers.27.self_attn.k_proj.weight]\u001b[A\n",
            "Loading weights:  99% 334/339 [00:38<00:00,  8.03it/s, Materializing param=model.layers.27.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  99% 334/339 [00:38<00:00,  8.03it/s, Materializing param=model.layers.27.self_attn.o_proj.weight]\u001b[A\n",
            "Loading weights:  99% 335/339 [00:38<00:00,  8.03it/s, Materializing param=model.layers.27.self_attn.q_proj.bias]  \u001b[A\n",
            "Loading weights:  99% 335/339 [00:38<00:00,  8.03it/s, Materializing param=model.layers.27.self_attn.q_proj.bias]\u001b[A\n",
            "Loading weights:  99% 336/339 [00:38<00:00,  8.03it/s, Materializing param=model.layers.27.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  99% 336/339 [00:38<00:00,  8.03it/s, Materializing param=model.layers.27.self_attn.q_proj.weight]\u001b[A\n",
            "Loading weights:  99% 337/339 [00:38<00:00,  8.03it/s, Materializing param=model.layers.27.self_attn.v_proj.bias]  \u001b[A\n",
            "Loading weights:  99% 337/339 [00:38<00:00,  8.03it/s, Materializing param=model.layers.27.self_attn.v_proj.bias]\u001b[A\n",
            "Loading weights: 100% 338/339 [00:38<00:00,  8.03it/s, Materializing param=model.layers.27.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights: 100% 338/339 [00:38<00:00,  8.03it/s, Materializing param=model.layers.27.self_attn.v_proj.weight]\u001b[A\n",
            "Loading weights: 100% 339/339 [00:38<00:00,  8.03it/s, Materializing param=model.norm.weight]                      \u001b[A\n",
            "Loading weights: 100% 339/339 [00:38<00:00,  8.74it/s, Materializing param=model.norm.weight]\n",
            "\n",
            "generation_config.json: 100% 243/243 [00:00<00:00, 1.70MB/s]\n",
            "Some parameters are on the meta device because they were offloaded to the cpu.\n",
            "\n",
            "[QA | SRC + BT] Wikivoyage_1:2344 (#q=3)\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "\n",
            "[QA | SRC + BT] PubMed_10:868 (#q=4)\n",
            "\n",
            "[QA | SRC + BT] CMU_1:100 (#q=3)\n",
            "\n",
            "[QA | SRC + BT] wiki_13:2523 (#q=3)\n",
            "\n",
            "[QA | SRC + BT] PubMed_10:1035 (#q=4)\n",
            "\n",
            "[QA | SRC + BT] PubMed_10:1004 (#q=3)\n",
            "\n",
            "[QA | SRC + BT] PubMed_10:982 (#q=4)\n",
            "\n",
            "[QA | SRC + BT] PubMed_10:896 (#q=3)\n",
            "\n",
            "[QA | SRC + BT] wiki_13:2518 (#q=4)\n",
            "\n",
            "[QA | SRC + BT] PubMed_10:858 (#q=3)\n",
            "\n",
            "[QA | SRC + BT] Wikivoyage_1:2382 (#q=3)\n",
            "\n",
            "[QA | SRC + BT] wiki_13:2522 (#q=4)\n",
            "\n",
            "[QA | SRC + BT] wiki_5:2783 (#q=4)\n",
            "\n",
            "[QA | SRC + BT] Wikivoyage_1:2248 (#q=3)\n",
            "\n",
            "[QA | SRC + BT] PubMed_9:843 (#q=4)\n",
            "\n",
            "[QA | SRC + BT] Wikivoyage_1:2294 (#q=3)\n",
            "\n",
            "[QA | SRC + BT] Wikipedia_handpicked_5:1971 (#q=4)\n",
            "\n",
            "[QA | SRC + BT] CMU_1:125 (#q=3)\n",
            "\n",
            "[QA | SRC + BT] CMU_1:112 (#q=2)\n",
            "\n",
            "[QA | SRC + BT] PubMed_9:849 (#q=1)\n",
            "\n",
            "[QA | SRC + BT] PubMed_10:977 (#q=4)\n",
            "\n",
            "[QA | SRC + BT] PubMed_10:992 (#q=4)\n",
            "\n",
            "[QA | SRC + BT] Wikivoyage_1:2207 (#q=3)\n",
            "\n",
            "[QA | SRC + BT] Wikivoyage_1:2306 (#q=4)\n",
            "\n",
            "[QA | SRC + BT] CMU_1:104 (#q=3)\n",
            "\n",
            "[QA | SRC + BT] Wikivoyage_1:2264 (#q=4)\n",
            "\n",
            "[QA | SRC + BT] PubMed_10:957 (#q=3)\n",
            "\n",
            "[QA | SRC + BT] wiki_13:2497 (#q=3)\n",
            "\n",
            "[QA | SRC + BT] Wikivoyage_1:2355 (#q=3)\n",
            "\n",
            "[QA | SRC + BT] Wikivoyage_1:2408 (#q=3)\n",
            "\n",
            "[QA | SRC + BT] Wikipedia_handpicked_5:1968 (#q=4)\n",
            "\n",
            "[QA | SRC + BT] PubMed_10:979 (#q=4)\n",
            "\n",
            "[QA | SRC + BT] Wikisource_1:2051 (#q=4)\n",
            "\n",
            "[QA | SRC + BT] Wikivoyage_1:2293 (#q=3)\n",
            "\n",
            "[QA | SRC + BT] PubMed_10:1038 (#q=3)\n",
            "\n",
            "[QA | SRC + BT] wiki_11:2637 (#q=3)\n",
            "\n",
            "[QA | SRC + BT] wiki_5:2760 (#q=3)\n",
            "\n",
            "[QA | SRC + BT] CMU_1:28 (#q=2)\n",
            "\n",
            "[QA | SRC + BT] wiki_13:2541 (#q=4)\n",
            "\n",
            "[QA | SRC + BT] wiki_11:2634 (#q=3)\n",
            "\n",
            "[QA | SRC + BT] PubMed_10:917 (#q=4)\n",
            "\n",
            "[QA | SRC + BT] Wikivoyage_1:2404 (#q=4)\n",
            "\n",
            "[QA | SRC + BT] Wikipedia_handpicked_5:1887 (#q=3)\n",
            "\n",
            "[QA | SRC + BT] PubMed_10:913 (#q=3)\n",
            "\n",
            "[QA | SRC + BT] PubMed_10:974 (#q=3)\n",
            "\n",
            "[QA | SRC + BT] wiki_13:2545 (#q=4)\n",
            "\n",
            "[QA | SRC + BT] Wikipedia_handpicked_5:1883 (#q=4)\n",
            "\n",
            "[QA | SRC + BT] PubMed_9:848 (#q=3)\n",
            "\n",
            "[QA | SRC + BT] Wikipedia_handpicked_5:1928 (#q=4)\n",
            "\n",
            "[QA | SRC + BT] PubMed_9:853 (#q=1)\n",
            "\n",
            "✅ QA completed. done=50, skipped=0\n",
            "Download complete: 100% 15.2G/15.2G [37:20<00:00, 6.80MB/s]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!python ASKQE-Hallucination2/qa.py \\\n",
        "  --input_path /content/ASKQE-Hallucination2/data/contratico_expansion_bt_qg.jsonl \\\n",
        "  --output_path /content/ASKQE-Hallucination2/data/contratico_expansion_bt_qg_qa.jsonl \\\n",
        " --prompt_path /content/ASKQE-Hallucination2/prompt.json\\\n",
        "--prompt_key qa_prompt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EVALUATION"
      ],
      "metadata": {
        "id": "vJgfXj6twMD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!python /content/ASKQE-Hallucination2/ucr.py \\\n",
        "--input_path /content/ASKQE-Hallucination2/data/contratico_expansion_bt_qg_qa.jsonl\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIUNtUtawNa3",
        "outputId": "f3da95f2-fe37-44b3-a8b3-c2b306e29557"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[STEP] Loading dataset: /content/ASKQE-Hallucination2/data/contratico_expansion_bt_qg_qa.jsonl\n",
            "[OK] Loaded 50 samples\n",
            "\n",
            "====== UCR RESULTS ======\n",
            "Items total in file         : 50\n",
            "Items valid (used)          : 50\n",
            "Items skipped               : 0\n",
            "--------------------------------\n",
            "Questions flagged           : 20/165  (0.1212)\n",
            "Sentences flagged           : 16/50  (0.3200)\n",
            "================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/ASKQE-Hallucination2/simScores.py \\\n",
        "  --input_path /content/ASKQE-Hallucination2/data/contratico_expansion_bt_qg_qa.jsonl \\\n",
        "  --output_path /content/ASKQE-Hallucination2/data/mismatches.jsonl \\\n",
        "  --tau 0.6 --only_mismatches --include_context\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMKb4nnNPFTD",
        "outputId": "7e70bad1-85c4-4645-f0c9-3c151d3019c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Loading embedding model: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
            "modules.json: 100% 229/229 [00:00<00:00, 1.12MB/s]\n",
            "config_sentence_transformers.json: 100% 122/122 [00:00<00:00, 717kB/s]\n",
            "README.md: 3.89kB [00:00, 13.9MB/s]\n",
            "sentence_bert_config.json: 100% 53.0/53.0 [00:00<00:00, 400kB/s]\n",
            "config.json: 100% 645/645 [00:00<00:00, 3.50MB/s]\n",
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "model.safetensors: 100% 471M/471M [00:02<00:00, 190MB/s]\n",
            "Loading weights: 100% 199/199 [00:00<00:00, 901.59it/s, Materializing param=pooler.dense.weight]\n",
            "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
            "Key                     | Status     |  | \n",
            "------------------------+------------+--+-\n",
            "embeddings.position_ids | \u001b[38;5;208mUNEXPECTED\u001b[0m |  | \n",
            "\n",
            "\u001b[3mNotes:\n",
            "- \u001b[38;5;208mUNEXPECTED\u001b[0m\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
            "tokenizer_config.json: 100% 526/526 [00:00<00:00, 2.92MB/s]\n",
            "tokenizer.json: 100% 9.08M/9.08M [00:00<00:00, 37.9MB/s]\n",
            "special_tokens_map.json: 100% 239/239 [00:00<00:00, 1.64MB/s]\n",
            "config.json: 100% 190/190 [00:00<00:00, 1.18MB/s]\n",
            "[INFO] Read examples          : 50\n",
            "[INFO] Valid examples         : 50\n",
            "[INFO] Written examples       : 21\n",
            "[INFO] Pairs total (incl NA)  : 165\n",
            "[INFO] Pairs compared         : 126\n",
            "[SIM]  Mean sim (global)      : 0.7923\n",
            "[SIM]  Disagree rate (sim<tau): 0.1984\n",
            "[OK] Output -> /content/ASKQE-Hallucination2/data/mismatches.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CHECK"
      ],
      "metadata": {
        "id": "I153kMHJyc0L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer-aware Question Generation"
      ],
      "metadata": {
        "id": "yJbZXDxH29H0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -u /content/ASKQE-Hallucination2/checkqg.py \\\n",
        "  --input_path /content/ASKQE-Hallucination2/data/contratico_expansion_bt_qg_qa.jsonl \\\n",
        "  --output_path /content/ASKQE-Hallucination2/data/contratico_expansion_bt_qg_qa_checkqg.jsonl \\\n",
        "  --prompt_path /content/ASKQE-Hallucination2/prompt.json \\\n",
        "  --prompt_key qg_prompt_check"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiZCbGzNyeeG",
        "outputId": "e1a863f8-c53f-4354-8a90-65fcf24be56a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.12/pathlib.py\", line 555, in drive\n",
            "    return self._drv\n",
            "           ^^^^^^^^^\n",
            "AttributeError: 'PackagePath' object has no attribute '_drv'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 999, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\", line 45, in <module>\n",
            "    PACKAGE_DISTRIBUTION_MAPPING = importlib.metadata.packages_distributions()\n",
            "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/importlib/metadata/__init__.py\", line 947, in packages_distributions\n",
            "    for pkg in _top_level_declared(dist) or _top_level_inferred(dist):\n",
            "                                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/importlib/metadata/__init__.py\", line 958, in _top_level_inferred\n",
            "    f.parts[0] if len(f.parts) > 1 else inspect.getmodulename(f)\n",
            "                      ^^^^^^^\n",
            "  File \"/usr/lib/python3.12/pathlib.py\", line 706, in parts\n",
            "    if self.drive or self.root:\n",
            "       ^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/pathlib.py\", line 557, in drive\n",
            "    self._load_parts()\n",
            "  File \"/usr/lib/python3.12/pathlib.py\", line 415, in _load_parts\n",
            "    drv, root, tail = self._parse_path(path)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/pathlib.py\", line 404, in _parse_path\n",
            "    parsed = [sys.intern(str(x)) for x in rel.split(sep) if x and x != '.']\n",
            "              ^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/ASKQE-Hallucination2/checkqg.py\", line 1, in <module>\n",
            "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/__init__.py\", line 30, in <module>\n",
            "    from . import dependency_versions_check\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/dependency_versions_check.py\", line 16, in <module>\n",
            "    from .utils.versions import require_version, require_version_core\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/__init__.py\", line 22, in <module>\n",
            "    from .auto_docstring import (\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/auto_docstring.py\", line 30, in <module>\n",
            "    from .generic import ModelOutput\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\", line 33, in <module>\n",
            "    from .import_utils import is_mlx_available, is_torch_available, is_torch_fx_proxy, requires\n",
            "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1334, in _find_and_load_unlocked\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## YES/NO answer on src"
      ],
      "metadata": {
        "id": "kbLkIpnB3AeF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -u /content/ASKQE-Hallucination2/checkqa.py \\\n",
        "  --input_path  /content/ASKQE-Hallucination2/data/contratico_expansion_bt_qg_qa_checkqg.jsonl \\\n",
        "  --output_path /content/ASKQE-Hallucination2/data/contratico_expansion_bt_qg_qa_checkqg_checkqa.jsonl \\\n",
        "  --prompt_path /content/ASKQE-Hallucination2/prompt.json\\\n",
        "  --prompt_key qa_prompt_check"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BA94ijfJ08nJ",
        "outputId": "77aa70cf-2c2d-4d7a-f0c8-4dfd7ecc53bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config.json: 100% 663/663 [00:00<00:00, 3.98MB/s]\n",
            "tokenizer_config.json: 7.30kB [00:00, 24.4MB/s]\n",
            "vocab.json: 2.78MB [00:00, 62.2MB/s]\n",
            "merges.txt: 1.67MB [00:00, 93.1MB/s]\n",
            "tokenizer.json: 7.03MB [00:00, 94.5MB/s]\n",
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "model.safetensors.index.json: 27.8kB [00:00, 80.2MB/s]\n",
            "Downloading (incomplete total...): 0.00B [00:00, ?B/s]\n",
            "Downloading (incomplete total...): 100% 15.2G/15.2G [03:58<00:00, 73.6MB/s]\n",
            "Fetching 4 files: 100% 4/4 [03:58<00:00, 59.52s/it] \n",
            "Download complete: 100% 15.2G/15.2G [03:58<00:00, 63.9MB/s]\n",
            "Loading weights: 100% 339/339 [00:42<00:00,  8.05it/s, Materializing param=model.norm.weight]\n",
            "generation_config.json: 100% 243/243 [00:00<00:00, 1.10MB/s]\n",
            "Some parameters are on the meta device because they were offloaded to the cpu.\n",
            "[YES/NO QA] Wikivoyage_1:2344\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "  Q   : Is the list exhaustive?\n",
            "  BT  : No\n",
            "  SRC : No\n",
            "  Q   : Are these health restrictions found in China?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Is the answer non-exhaustive?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "[YES/NO QA] PubMed_10:868\n",
            "  Q   : Can we achieve doubling data extraction by doing it twice per week?\n",
            "  BT  : No\n",
            "  SRC : Yes\n",
            "  Q   : Can costs be reduced if necessary?\n",
            "  BT  : Yes\n",
            "  SRC : No\n",
            "  Q   : Can data extraction be increased per week?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : [\"Can costs be reduced?\"?]\n",
            "  BT  : Yes\n",
            "  SRC : No\n",
            "[YES/NO QA] CMU_1:100\n",
            "  Q   : Will I send you a picture?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Will you send the picture quickly?\n",
            "  BT  : Yes\n",
            "  SRC : No\n",
            "  Q   : Will you send a picture?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "[YES/NO QA] wiki_13:2523\n",
            "  Q   : Were seven trials evaluated for re-adapted and approved drugs for malaria?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Were hydroxychloroquine and chlorophyll phosphate among the drugs studied in four of the trials?\n",
            "  BT  : Yes\n",
            "  SRC : No\n",
            "  Q   : Are the drugs being evaluated for malaria?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "[YES/NO QA] PubMed_10:1035\n",
            "  Q   : Did the enhanced surveillance demonstrate rapid implementation and effectiveness?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Was a confirmed case of COVID-19 detected early?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Is the surveillance described as enhanced?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Was the enhanced surveillance implemented on a national network?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "[YES/NO QA] PubMed_10:1004\n",
            "  Q   : Are they using the SIR model for their adaptation?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Are they using approximate Bayesian calculation?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Are they comparing their results to weekly cases?\n",
            "  BT  : Yes\n",
            "  SRC : No\n",
            "[YES/NO QA] PubMed_10:982\n",
            "  Q   : Are the RCGP and RSC involved in the annual virological surveillance of influenza?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Are IVRI and other respiratory diseases mentioned as separate entities?\n",
            "  BT  : Yes\n",
            "  SRC : No\n",
            "  Q   : Are the RCGP RSC offices collecting samples from patients exhibiting symptoms of IVRI and other respiratory illnesses?\n",
            "  BT  : Yes\n",
            "  SRC : No\n",
            "  Q   : Does the answer list both RCGP and RSC as the initials of the organizations conducting the surveillance?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "[YES/NO QA] PubMed_10:896\n",
            "  Q   : Would we work in isolation?\n",
            "  BT  : No\n",
            "  SRC : No\n",
            "  Q   : Are you working on this investigation?\n",
            "  BT  : No\n",
            "  SRC : No\n",
            "  Q   : Is coordination mentioned as the state?\n",
            "  BT  : Yes\n",
            "  SRC : No\n",
            "[YES/NO QA] wiki_13:2518\n",
            "  Q   : Most new medicines of medical interest fail during drug development?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Some new medicines fail during drug development because they have an unacceptable toxicity?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Some new medicines fail during drug development because they are not effective in treating the target disease and cause side effects?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Are Phase II and III clinical trials mentioned?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Are NCEs short for New Chemical Entities?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "[YES/NO QA] PubMed_10:858\n",
            "  Q   : Has the RCGP RSC expanded its surveillance?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Does the surveillance monitor the temporal distribution of COVID-19 infection in the community?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Is containment and mitigation the strategy being evaluated?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "[YES/NO QA] Wikivoyage_1:2382\n",
            "  Q   : Is a connection to an affected area a factor that could lead to subsequent entry restrictions?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Some countries will not allow changing planes from an affected area and require mandatory quarantine?\n",
            "  BT  : Yes\n",
            "  SRC : No\n",
            "  Q   : Will some countries allow changing planes after traveling to an affected area?\n",
            "  BT  : No\n",
            "  SRC : No\n",
            "[YES/NO QA] wiki_13:2522\n",
            "  Q   : Was the total capital expenditure $2.6 billion?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Did the manufacturer's drug succeed in phase III trials?\n",
            "  BT  : Yes\n",
            "  SRC : No\n",
            "  Q   : Did the manufacturer's drug become a commercial success?\n",
            "  BT  : Yes\n",
            "  SRC : No\n",
            "  Q   : Was the dollar value of the total capital expenditure in 2013 $2.6 billion?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Is the annual rate of increase for the total capital expenditure 8.5 percent?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Was the analysis in 2016 conducted on 106 medicines?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "[YES/NO QA] wiki_5:2783\n",
            "  Q   : Is it speculated that the 1890 flu pandemic may have been caused by an interspecific jump event?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Did the answer state that the 1890 flu pandemic was caused by an interspecific jump event rather than the influenza virus?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Are the time of onset, neurological symptoms, and cases of pneumonia mentioned as related factors for the cause of the pandemic?\n",
            "  BT  : Yes\n",
            "  SRC : No\n",
            "  Q   : Is the unknown causative agent not specified in the sentence?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "[YES/NO QA] Wikivoyage_1:2248\n",
            "  Q   : Could you not take a step in their direction while trying to touch the other person's hand?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "[YES/NO QA] PubMed_9:843\n",
            "  Q   : Do the measures include washing hands?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Do the measures include cleaning and disinfecting frequently contacted areas?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Is staying at home considered a measure to protect oneself from COVID-19?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Does the answer include avoiding travel?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Should people avoid contact with people who are sick to protect themselves from COVID-19?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Should people wear a mask in public places?\n",
            "  BT  : Yes\n",
            "  SRC : No\n",
            "[YES/NO QA] Wikivoyage_1:2294\n",
            "  Q   : Should you use disinfectant towels after washing your hands?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Should you use disinfectant towels before sitting down?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Should you clean the area around your seat with disinfectant towels?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Should you clean the area around your table with disinfectant towels?\n",
            "  BT  : Yes\n",
            "  SRC : No\n",
            "[YES/NO QA] Wikipedia_handpicked_5:1971\n",
            "  Q   : Is this about integrated digital learning platforms, video classes and CEMA?\n",
            "  BT  : Yes\n",
            "  SRC : No\n",
            "  Q   : Can content be broadcast over radio?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Can content be broadcast over the internet?\n",
            "  BT  : Yes\n",
            "  SRC : No\n",
            "  Q   : Can content be broadcast over television?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Are integrated digital learning platforms mentioned?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Are video classes mentioned?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Is CEMA mentioned?\n",
            "  BT  : Yes\n",
            "  SRC : No\n",
            "  Q   : Broadcast can be done over the internet?\n",
            "  BT  : Yes\n",
            "  SRC : No\n",
            "  Q   : Broadcast can be done over radio?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Broadcast can be done over television?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "[YES/NO QA] CMU_1:125\n",
            "  Q   : Did the high fever start two days ago?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Is the symptom high fever?\n",
            "  BT  : Yes\n",
            "  SRC : No\n",
            "[YES/NO QA] CMU_1:112\n",
            "  Q   : Are high blood pressure and diabetes mentioned as health conditions?\n",
            "  BT  : Yes\n",
            "  SRC : No\n",
            "  Q   : Is there no information provided about associations with high blood pressure and diabetes?\n",
            "  BT  : No\n",
            "  SRC : No\n",
            "[YES/NO QA] PubMed_9:849\n",
            "  Q   : Is it stated that no answer is known?\n",
            "  BT  : No\n",
            "  SRC : No\n",
            "[YES/NO QA] PubMed_10:977\n",
            "  Q   : A small number of cases can help develop a test kit for patients?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Can the test kit only be taken to their own general practice?\n",
            "  BT  : No\n",
            "  SRC : No\n",
            "  Q   : Is acceptability to patients explored regarding the test kit?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Is the test kit used to promote the effectiveness of treatment?\n",
            "  BT  : Yes\n",
            "  SRC : No\n",
            "[YES/NO QA] PubMed_10:992\n",
            "  Q   : Does the policy seek to reduce the number of cases to low levels?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Does the policy aim to control transmission from person to person?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Is the target value for R less than 1?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Is R defined as the average number of secondary cases generated by each case?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Were severe acute respiratory syndrome and Ebola mentioned as examples of previous outbreaks?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "[YES/NO QA] Wikivoyage_1:2207\n",
            "  Q   : Is pneumonia listed as a serious complication of the disease?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Serious complications can cause disability?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Serious complications can cause death?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "[YES/NO QA] Wikivoyage_1:2306\n",
            "  Q   : Is the purpose of following up to test or quarantine them?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Can the authorities follow up with the people who were sitting near the infected passenger?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Can the authorities follow up with the rest of the passengers?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Can you test the passengers?\n",
            "  BT  : Yes\n",
            "  SRC : No\n",
            "  Q   : Can you quarantine the passengers?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "[YES/NO QA] CMU_1:104\n",
            "  Q   : Is this kind of severe chest pain mentioned in the original source text?\n",
            "  BT  : Yes\n",
            "  SRC : No\n",
            "  Q   : Is almost every occurrence of this chest pain severe?\n",
            "  BT  : Yes\n",
            "  SRC : No\n",
            "  Q   : Is severe chest pain almost always indicative of a serious condition?\n",
            "  BT  : Yes\n",
            "  SRC : No\n",
            "[YES/NO QA] Wikivoyage_1:2264\n",
            "  Q   : Is wearing a mask mandatory in some countries and cities?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Is a mask mentioned as something used to protect public health?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Are masks mentioned as a method to reduce community transmission?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Do some countries and cities differ in the use of masks?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "[YES/NO QA] PubMed_10:957\n",
            "  Q   : Can only clinical staff do so?\n",
            "  BT  : No\n",
            "  SRC : No\n",
            "  Q   : Can administrative staff do so?\n",
            "  BT  : Yes\n",
            "  SRC : No\n",
            "  Q   : Can it be done at the reception?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Is the answer based on the protocol of the individual office?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "[YES/NO QA] wiki_13:2497\n",
            "  Q   : [\"Do these have anything?\"?]\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Is the activity related to a biological target?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Is the disease related to COVID-19?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "[YES/NO QA] Wikivoyage_1:2355\n",
            "  Q   : Is entry into apartment complexes banned?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Is the ban applied to visitors who do not temporarily reside there?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Are temporary residents exempt from the ban?\n",
            "  BT  : No\n",
            "  SRC : No\n",
            "[YES/NO QA] Wikivoyage_1:2408\n",
            "  Q   : Reputable doctors should back up the information and advice?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : All information should be backed up by reputable individuals?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : \n",
            "  BT  : No\n",
            "  SRC : No\n",
            "[YES/NO QA] Wikipedia_handpicked_5:1968\n",
            "  Q   : Did Cornell decide that students should stay home?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Was virtual teaching decided upon by Cornell?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Did Cornell call for immediate and forceful federal action?\n",
            "  BT  : No\n",
            "  SRC : No\n",
            "[YES/NO QA] PubMed_10:979\n",
            "  Q   : Does the process include checking the current PHE guideline on infectious considerations for confirmed cases?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Is offering the patient an appointment part of the process?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Is sending a reminder via text message included in the process?\n",
            "  BT  : Yes\n",
            "  SRC : No\n",
            "[YES/NO QA] Wikisource_1:2051\n",
            "  Q   : Are residents required to comply with current state public health directives?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Does the answer state that the health care system needs to serve everyone according to the order?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Did the Department of Public Health develop the current state public health directives?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Were the directives developed for the current statewide COVID-19 situation?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "[YES/NO QA] Wikivoyage_1:2293\n",
            "  Q   : Should you reserve a seat on the side of the window?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Should you avoid moving around the cabin during the flight?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Should you avoid talking to other passengers during the flight?\n",
            "  BT  : Yes\n",
            "  SRC : No\n",
            "[YES/NO QA] PubMed_10:1038\n",
            "  Q   : Is there any significant increase in risk?\n",
            "  BT  : No\n",
            "  SRC : No\n",
            "  Q   : Are the office staff or other patients mentioned in the answer?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Is it mentioned that there is no significant increase in risk for office staff?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "[YES/NO QA] wiki_11:2637\n",
            "  Q   : Is a possible case defined as one where the virus test is not conclusive?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Is a possible case defined as one where testing could not be carried out for any reason?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "[YES/NO QA] wiki_5:2760\n",
            "  Q   : Are MRNAs genetic transcripts?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Do MRNAs correspond to the last third of the viral genome?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : MRNAs include specific proteins?\n",
            "  BT  : Yes\n",
            "  SRC : No\n",
            "[YES/NO QA] CMU_1:28\n",
            "  Q   : Are the symptoms lasting for five business days?\n",
            "  BT  : No\n",
            "  SRC : No\n",
            "  Q   : Is the duration mentioned for the symptoms to go away five business days?\n",
            "  BT  : Yes\n",
            "  SRC : No\n",
            "[YES/NO QA] wiki_13:2541\n",
            "  Q   : Is the success rate of medicines in Phase I or II trials less than 12%?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Are the medicines in Phase I or II trials?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Is the minimum success rate for these medicines less than 12%?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Does passing all phases of the trial lead to final approval?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "[YES/NO QA] wiki_11:2634\n",
            "  Q   : Is COVID-19 surveillance expected to monitor epidemiological trends?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Is it stated that something will quickly detect new cases?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Will providing epidemiological data help assess risk and guide preparedness against the disease?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "[YES/NO QA] PubMed_10:917\n",
            "  Q   : SNOMED CT concepts for COVID-19 were made available in all CMR systems?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Did local hospitals receive printed copies of SNOMED CT concepts for COVID-19?\n",
            "  BT  : Yes\n",
            "  SRC : No\n",
            "  Q   : Did printed copies of SNOMED CT concepts for COVID-19 go to local hospitals?\n",
            "  BT  : Yes\n",
            "  SRC : No\n",
            "  Q   : Was the publication an emergency publication in the UK?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "[YES/NO QA] Wikivoyage_1:2404\n",
            "  Q   : Does the UK Government provide travel advice and restrictions?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : [\"Does the information come from the Foreign and Commonwealth Office?\"?, \"Does the information come from the Ministry of Foreign Affairs?\"]\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Is travel advice provided by the Foreign and Commonwealth Office?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Does the Ministry of Foreign Affairs provide travel restrictions?\n",
            "  BT  : No\n",
            "  SRC : No\n",
            "[YES/NO QA] Wikipedia_handpicked_5:1887\n",
            "  Q   : When is it recommended to extend the suspension of classes and extracurricular activities?\n",
            "  BT  : Yes\n",
            "  SRC : No\n",
            "  Q   : Should extending the suspension of classes and extracurricular activities be considered when transmission in the local community is significant?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Are social distancing strategies mentioned as a method?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "[YES/NO QA] PubMed_10:913\n",
            "  Q   : Does the NHS use Systematized Nomenclature of Medicine - Clinical Terms (SNOMED CT) for codification?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : SNOMED CT is usually updated twice a year?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : [\"Is it rigorously implied that SNOMED CT is mentioned?\"?]\n",
            "  BT  : Yes\n",
            "  SRC : No\n",
            "[YES/NO QA] PubMed_10:974\n",
            "  Q   : Will a new application form be prepared?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Do the offices need to record recent travel?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Do the offices need to record work exposure?\n",
            "  BT  : Yes\n",
            "  SRC : No\n",
            "  Q   : Do the offices need to record exposure to COVID-19?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Does the answer list recent travel as a type of exposure that needs to be recorded??\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Does the answer list work exposure as a type of exposure that needs to be recorded??\n",
            "  BT  : Yes\n",
            "  SRC : No\n",
            "  Q   : Does the answer list exposure to COVID-19 as a type of exposure that needs to be recorded?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "[YES/NO QA] wiki_13:2545\n",
            "  Q   : Do the drugs interact with prescription drugs?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Is the therapeutic dose affected by these interactions?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Is the answer about the mitigation of the disease?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : [\"Is the duration of treatment unrelated to any influence?\"?]\n",
            "  BT  : No\n",
            "  SRC : No\n",
            "[YES/NO QA] Wikipedia_handpicked_5:1883\n",
            "  Q   : Did they analyze the spread of the flu?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Flu cases decreased when schools were closed?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Flu cases reappeared when schools reopened?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Was the analysis conducted in France?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "[YES/NO QA] PubMed_9:848\n",
            "  Q   : Are community mitigation strategies aimed at curbing the spread of diseases?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Are those with underlying medical conditions especially protected by community mitigation strategies?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Are essential workers especially protected by community mitigation strategies?\n",
            "  BT  : Yes\n",
            "  SRC : No\n",
            "  Q   : Are others especially protected by community mitigation strategies?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Is serious illness mentioned as a key element in the original text?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "[YES/NO QA] Wikipedia_handpicked_5:1928\n",
            "  Q   : Does the increase in difficulty apply to students without home internet access?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Does the answer restate the question without providing new information?\n",
            "  BT  : No\n",
            "  SRC : No\n",
            "  Q   : Is the type of education mentioned distance education?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "  Q   : Is internet access needed but lacking for some students?\n",
            "  BT  : Yes\n",
            "  SRC : Yes\n",
            "[YES/NO QA] PubMed_9:853\n",
            "  Q   : This answer does not provide any information about possible consequences for public health practice?\n",
            "  BT  : No\n",
            "  SRC : No\n",
            "✅ Contrastive YES/NO QA on BT+SRC completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "9UZZvwVq3HYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/ASKQE-Hallucination2/compute_contrastive_metrics.py \\\n",
        "  --input_path /content/ASKQE-Hallucination2/data/contratico_expansion_bt_qg_qa_checkqg_checkqa.jsonl \\\n",
        "  --dump_path /content/ASKQE-Hallucination2/data/contrastive_src_no_bt_yes.jsonl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEy94fJs3IkE",
        "outputId": "9e4c4f76-6aee-49f5-84f0-6c026562d0af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "====== CONTRASTIVE (GATED BY BT==YES) ======\n",
            "Sentences (valid fields)              : 50\n",
            "Sentences with >=1 BT==YES question   : 48\n",
            "Sentences in gated denominator        : 48\n",
            "Total gated questions (BT==YES)       : 162\n",
            "SRC answers YES (on gated)            : 125\n",
            "SRC answers NO  (on gated)            : 37\n",
            "-------------------------------------------\n",
            "CHR_gated_question = P(SRC==NO|BT==YES): 0.2284\n",
            "CHR_gated_sentence (OR over questions): 0.5625\n",
            "[DUMP] Wrote 37 records -> /content/ASKQE-Hallucination2/data/contrastative_src_no_bt_yes.jsonl\n",
            "===========================================\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}